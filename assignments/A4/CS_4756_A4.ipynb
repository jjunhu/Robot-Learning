{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Due Date**\n",
        "April 9 at 11:59PM EST\n",
        "\n",
        "# **Introduction**\n",
        "\n",
        "Welcome to Assignment 4 of CS 4756/5756. In this assignment, you will implement several variations of Policy Gradient methods. Concretely, you will:\n",
        "* Implement the REINFORCE algorithm (Part 1)\n",
        "* Incorporate Reward-To-Go into the REINFORCE loss function (Part 2)\n",
        "* Implement the Advantage Actor-Critic algorithm (Extra Credit, required for CS 5756 students)\n",
        "\n",
        "You will use the CartPole-v1 environment for this assignment. Refer to the Gym website for more details about the [CartPole environment](https://www.gymlibrary.dev/environments/classic_control/cart_pole/).\n",
        "\n",
        "\n",
        "Please read through the following paragraphs carefully.\n",
        "\n",
        "**Getting Started:** You are free to complete this assignment on **either  [Google Colab](https://colab.research.google.com/) or your local machine**. Note that there will be a small amount of extra setup if you choose to complete the assignment on your local machine (see **Setup** section below).\n",
        "\n",
        "**Evaluation:**\n",
        "Your code will be tested for correctness and, for certain assignments, speed. For this particular assignment, performance results will not be harshly graded (although we provide approximate expected reward numbers, you are not expected to replicate them exactly). Please remember that all assignments should be completed individually.\n",
        "\n",
        "**Academic Integrity:** We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else’s code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don’t try. We trust you all to submit your own work only; please don’t let us down. If you do, we will pursue the strongest consequences available to us.\n",
        "\n",
        "**Getting Help:** The [Resources](https://www.cs.cornell.edu/courses/cs4756/2024sp/#resources) section on the course website is your friend! If you ever feel stuck in these projects, please feel free to avail yourself to office hours and Edstem! If you are unable to make any of the office hours listed, please let TAs know and we will be happy to assist. If you need a refresher for PyTorch, please see this [60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)! For Numpy, please see the quickstart [here](https://numpy.org/doc/stable/user/quickstart.html) and full API [here](https://numpy.org/doc/stable/reference/).\n"
      ],
      "metadata": {
        "id": "GugcHV0e5IKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**\n",
        "\n",
        "As mentioned above, you are free to use Google Colab or your local machine for this assignment. Regardless of your choice, **you will need to run the cell below**. If you are using your local machine, though, you will first need to set up a conda environment that contains the packages found in requirements.txt.\n",
        "\n",
        "\n",
        "### Setting Up Conda Environment (Local Machine Only)\n",
        "\n",
        "In order to complete the assignment locally, you will need to install the required libraries. To do this, we will use the package manager [Conda](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html).\n",
        "\n",
        "* First, create a Conda environment in the terminal with the correct version of python by running: `conda create --name cs4756_a4 python=3.10`\n",
        "* Next, activate the environment by running: `conda activate cs4756_a4`\n",
        "* Lastly, install the required libraries by runnning: `pip install -r requirements.txt`\n",
        "\n",
        "When you run the notebook, make sure to set the Python interpreter and kernel to be the version of python from the `cs4756_a4` environment. If you are using VSCode, you may need to restart after creating the environment in order for `cs4756_a4` to be a visible option that you can select for your kernel.\n",
        "\n"
      ],
      "metadata": {
        "id": "d52C6fHT9eR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import sys\n",
        "USING_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if USING_COLAB:\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "    !pip install -U colabgymrender\n",
        "    !pip install -U moviepy==0.2.3.5\n",
        "    !pip install imageio==2.4.1\n",
        "    !pip install --upgrade AutoROM\n",
        "    !AutoROM --accept-license\n",
        "    !pip install gym[classic_control] > /dev/null 2>&1\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "import random\n",
        "\n",
        "\n",
        "seed = 695\n",
        "\n",
        "# Setting the seed to ensure reproducability\n",
        "def reseed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "reseed(seed)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEa_6lD3-OHI",
        "outputId": "4d084f77-aaf7-4bed-a3ad-ce3b634bed43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Helper Function\n",
        "\n",
        "Below, we provide the helper function `visualize` for your use. This function will create a visualization of the CartPole environment. If you are using Colab, calling this function will render the visualization within the notebook. If you are using your local machine, this function will instead save a video of the visualization to your current directory (rendering videos in Jupyter Notebooks is not widely supported outside of Colab)."
      ],
      "metadata": {
        "id": "Wq62xgt0_q_3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def visualize(algorithm=None, video_name=\"test\"):\n",
        "    \"\"\"Visualize a policy network for a given algorithm on a single episode\n",
        "\n",
        "        Args:\n",
        "            algorithm (PolicyGradient): Algorithm whose policy network will be rolled out for the episode. If\n",
        "            no algorithm is passed in, a random policy will be visualized.\n",
        "            video_name (str): Name for the mp4 file of the episode that will be saved (omit .mp4). Only used\n",
        "            when running on local machine.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_action(obs):\n",
        "        if not algorithm:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            return algorithm.select_action(obs)\n",
        "\n",
        "    if USING_COLAB:\n",
        "        from colabgymrender.recorder import Recorder\n",
        "\n",
        "        directory = './video'\n",
        "        env = gym.make('CartPole-v1')\n",
        "        env = Recorder(env, directory)\n",
        "        obs = env.reset()\n",
        "        for i in range(500):\n",
        "            action = get_action(obs)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        env.play()\n",
        "    else:\n",
        "        import cv2\n",
        "\n",
        "        video = cv2.VideoWriter(f\"{video_name}.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), 24, (600,400))\n",
        "        env = gym.make(\"CartPole-v1\")\n",
        "        obs = env.reset()\n",
        "        for i in range(500):\n",
        "            action = get_action(obs)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            im = env.render(mode='rgb_array')\n",
        "            im = im[:,:,::-1]\n",
        "\n",
        "            video.write(im)\n",
        "\n",
        "        video.release()\n",
        "        env.close()\n",
        "        print(f\"Video saved as {video_name}.mp4\")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlAWmVPoBJz4",
        "outputId": "45940b0f-a657-4477-f144-e8ca550d38b3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing CartPole-v1\n",
        "\n",
        "CartPole-v1 is a classic control problem in the field of reinforcement learning. The task is to balance a pole on a cart by moving the cart left or right. The environment consists of a cart that can move horizontally along a track, and a pole that is attached to the cart by a hinge. The pole can rotate freely around the hinge, and the goal is to keep the pole balanced by moving the cart to keep the pole upright.\n",
        "\n",
        "The observation space consists of four variables: the horizontal position and velocity of the cart, and the angle and angular velocity of the pole. The action space consists of two discrete actions: move the cart to the left, or move the cart to the right. The episode ends when the pole falls beyond a certain angle, or the cart moves too far from the center.\n",
        "\n",
        "**Run the cell below to visualize the CartPole-v1 environment:**"
      ],
      "metadata": {
        "id": "u322aMAspfpr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "visualize()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "HoqC_reVB9hM",
        "outputId": "a8b01570-9690-4f37-80e6-2e20f7d6a9bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Vanilla REINFORCE\n",
        "\n",
        "In this assignment, we will implement the REINFORCE algorithm and apply it to CartPole-v1. This algorithm is a popular reinforcement learning algorithm that can be used for both discrete and continuous action spaces.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The REINFORCE algorithm uses a neural network to learn a policy that maps states to actions. The algorithm collects a set of trajectories, which are used to update the policy network. Below, we present a brief overview of REINFORCE and the equations behind it. **See MACRL 11.4 for a full description of REINFORCE and derivations for the following equations.**\n",
        "\n",
        "To start, recall the reinforcement learning objective:\n",
        "$$ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[r(\\tau)] $$\n",
        "\n",
        "\n",
        "The goal of REINFORCE is to maximize $J(\\theta)$. As REINFORCE is a policy gradient algorithm, it involves taking the gradient of this objective with respect to the parameters $\\theta$ of the policy $\\pi_{\\theta}$:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\nabla_θ log \\pi_\\theta (\\tau) r(\\tau)] $$\n",
        "\n",
        "The REINFORCE algorithm approximates this quantity from N trajectories as follows:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\Biggl( \\sum_{t=0}^{T-1} \\nabla_\\theta log \\pi_\\theta(a_{it}|s_{it})\\Biggl) \\Biggl(\\sum_{t=0}^{T-1} r(s_{it}, a_{it}) \\Biggl) $$\n",
        "\n",
        "\n",
        "**Note:** You will implement a slightly modified version of REINFORCE that uses the total discounted reward (with discount factor $\\gamma$) rather than the total reward in order to encourage the agent to prioritize more immediate rewards over those in the distant future:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\Biggl( \\sum_{t=0}^{T-1} \\nabla_\\theta log \\pi_\\theta(a_{it}|s_{it})\\Biggl) \\Biggl(\\sum_{t=0}^{T-1} \\gamma^{t} r(s_{it}, a_{it}) \\Biggl) $$\n",
        "\n",
        "As the goal of REINFORCE is to maximize $J(\\theta)$, our loss function $L(\\theta)$ (which we will be minimizing) will be the following:\n",
        "\n",
        "$$ L(\\theta) = - \\frac{1}{N} \\sum_{i=1}^{N} \\Biggl( \\sum_{t=0}^{T-1} log \\pi_\\theta(a_{it}|s_{it})\\Biggl) \\Biggl(\\sum_{t=0}^{T-1} \\gamma^{t} r(s_{it}, a_{it}) \\Biggl)$$\n",
        "\n",
        "\n",
        "\n",
        "## Instructions\n",
        "\n",
        "You will need to implement the following:\n",
        "\n",
        "1. `PolicyNet` class - This class will define the policy network used in the REINFORCE algorithm.\n",
        "\n",
        "2. `PolicyGradient` class - This class will define the REINFORCE algorithm.\n",
        "\n",
        "\n",
        "Follow the instructions below to implement each of these components.\n",
        "\n",
        "### `PolicyNet` class\n",
        "\n",
        "The `PolicyNet` class should define a neural network that takes in a state and outputs a probability distribution over the action space. The network should have the following architecture:\n",
        "\n",
        "- Input layer: a fully-connected layer with `state_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Output layer: a fully-connected layer with `hidden_dim` input nodes and `action_dim` output nodes, followed by a softmax activation function.\n",
        "\n",
        "You should use the `nn` module of PyTorch to define this network."
      ],
      "metadata": {
        "id": "7H9zQBy_zg0X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int):\n",
        "        \"\"\"Policy network for the REINFORCE algorithm.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): Dimension of the state space.\n",
        "            action_dim (int): Dimension of the action space.\n",
        "            hidden_dim (int): Dimension of the hidden layers.\n",
        "        \"\"\"\n",
        "        super(PolicyNet, self).__init__()\n",
        "        # TODO: Implement the policy network for the REINFORCE algorithm here\n",
        "\n",
        "    def forward(self, state: torch.Tensor):\n",
        "        \"\"\"Forward pass of the policy network.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): State of the environment.\n",
        "\n",
        "        Returns:\n",
        "            x (torch.Tensor): Probabilities of the actions.\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass of the policy network here\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "ErpoIXHZU_nK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### `PolicyGradient` class\n",
        "\n",
        "The `PolicyGradient` class should define the REINFORCE algorithm.\n",
        "The `PolicyGradient` class should have the following methods:\n",
        "\n",
        "- `__init__(self, env, policy_net, seed, reward_to_go: bool = False)`: Constructor method that initializes the environment and policy network.\n",
        "\n",
        "- `select_action(self, state)`: Method that selects an action based on the policy network.\n",
        "\n",
        "- `compute_loss(self, episode, gamma)`: Method that computes the loss for a given episode.\n",
        "\n",
        "- `update_policy(self, episodes, optimizer, gamma)`: Method that updates the policy network.\n",
        "\n",
        "- `train(self, num_iterations, batch_size, gamma, lr)`: Method that trains the policy network using the REINFORCE algorithm.\n",
        "\n",
        "- `run_episode(self)`: Method that runs an episode of the environment and returns the episode.\n",
        "\n",
        "- `evaluate(self, num_episodes = 100)`: Method that evaluates the policy network by running multiple episodes."
      ],
      "metadata": {
        "id": "CAZMJx-eqLpm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class PolicyGradient:\n",
        "    def __init__(self, env, policy_net, seed, reward_to_go: bool = False):\n",
        "        \"\"\"Policy gradient algorithm based on the REINFORCE algorithm.\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): Environment\n",
        "            policy_net (PolicyNet): Policy network\n",
        "            seed (int): Seed\n",
        "            reward_to_go (bool): True if using reward_to_go, False if not\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.policy_net = policy_net.to(self.device)\n",
        "        self.reward_to_go = reward_to_go\n",
        "        self.seed = seed\n",
        "        self.env.seed(self.seed)\n",
        "        self.env.action_space.seed(self.seed)\n",
        "        self.env.observation_space.seed(self.seed)\n",
        "        torch.manual_seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action based on the policy network\n",
        "\n",
        "        Args:\n",
        "            state (np.ndarray): State of the environment\n",
        "\n",
        "        Returns:\n",
        "            action (int): Action to be taken\n",
        "        \"\"\"\n",
        "        # TODO: Implement the action selection here based on the policy network output probabilities\n",
        "        # Hint: You can use torch.distributions.Categorical to sample from the policy network output distribution\n",
        "\n",
        "    def compute_loss(self, episode, gamma):\n",
        "        \"\"\"Compute the loss function for the REINFORCE algorithm\n",
        "\n",
        "        Args:\n",
        "            episode (list): List of tuples (state, action, reward)\n",
        "            gamma (float): Discount factor\n",
        "\n",
        "        Returns:\n",
        "            loss (torch.Tensor): The value of the loss function\n",
        "        \"\"\"\n",
        "        # TODO: Extract states, actions and rewards from the episode\n",
        "\n",
        "        if not self.reward_to_go:\n",
        "            # TODO: Part 1: Compute the total discounted reward here\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "            # TODO: Part 2: Compute the discounted rewards to go here\n",
        "            pass\n",
        "\n",
        "        # TODO: Implement the loss function for the REINFORCE algorithm here\n",
        "        loss = None\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def update_policy(self, episodes, optimizer, gamma):\n",
        "        \"\"\"Update the policy network using the batch of episodes\n",
        "\n",
        "        Args:\n",
        "            episodes (list): List of episodes\n",
        "            optimizer (torch.optim): Optimizer\n",
        "            gamma (float): Discount factor\n",
        "        \"\"\"\n",
        "        # TODO: Compute the loss function for each episode using compute_loss\n",
        "\n",
        "        # TODO: Update the policy network using average loss across the batch\n",
        "\n",
        "        pass\n",
        "\n",
        "    def run_episode(self):\n",
        "        \"\"\"\n",
        "        Run an episode of the environment and return the episode\n",
        "\n",
        "        Returns:\n",
        "            episode (list): List of tuples (state, action, reward)\n",
        "        \"\"\"\n",
        "        self.env.seed(seed)\n",
        "        self.env.action_space.seed(seed)\n",
        "        self.env.observation_space.seed(seed)\n",
        "        state = self.env.reset()\n",
        "        episode = []\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "        return episode\n",
        "\n",
        "    def train(self, num_iterations, batch_size, gamma, lr):\n",
        "        \"\"\"Train the policy network using the REINFORCE algorithm\n",
        "\n",
        "        Args:\n",
        "            num_iterations (int): Number of iterations to train the policy network\n",
        "            batch_size (int): Number of episodes per batch\n",
        "            gamma (float): Discount factor\n",
        "            lr (float): Learning rate\n",
        "        \"\"\"\n",
        "        self.policy_net.train()\n",
        "        optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        # TODO: Implement the training loop for the REINFORCE algorithm here. \n",
        "        # Update the policy every iteration, and use one batch per iteration.\n",
        "\n",
        "    def evaluate(self, num_episodes = 100):\n",
        "        \"\"\"Evaluate the policy network by running multiple episodes.\n",
        "\n",
        "        Args:\n",
        "            num_episodes (int): Number of episodes to run\n",
        "\n",
        "        Returns:\n",
        "            average_reward (float): Average total reward per episode\n",
        "        \"\"\"\n",
        "        self.policy_net.eval()\n",
        "        # TODO: Implement an evaluation loop for the REINFORCE algorithm here\n",
        "        # by running multiple episodes and returning the average total reward"
      ],
      "outputs": [],
      "metadata": {
        "id": "NHA2HN-AqM-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have implemented REINFORCE, it is time to train and evaluate a policy. Below, we provide training hyperparameters which you should use in your experiments. See the writeup for additional instructions on what metrics and figures you need to include in your submission.\n",
        "\n",
        "* Expected Training Time (Colab CPU): 5 minutes\n",
        "* Expected Reward (when calling `evaluate(100)`): 50-100"
      ],
      "metadata": {
        "id": "uVULWyG8R1KO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Feel free to use the space below to run experiments and create plots used in your writeup.\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "env.observation_space.seed(seed)\n",
        "\n",
        "policy_net = PolicyNet(env.observation_space.shape[0], env.action_space.n, 128)\n",
        "\n",
        "reinforce = PolicyGradient(env, policy_net, seed, reward_to_go=False)\n",
        "reinforce.train(num_iterations=200, batch_size=10, gamma=0.99, lr=0.001)\n",
        "\n",
        "visualize(algorithm=reinforce, video_name=\"reinforce\")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WSQTxjO6rxKQ",
        "outputId": "cde55300-0f0e-40bc-c1c6-4fbdc335c73d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: REINFORCE with Reward-to-Go\n",
        "\n",
        "In this part of the assignment, we will modify the REINFORCE algorithm to use the (discounted) reward-to-go instead of the total discounted reward.\n",
        "\n",
        "To decrease the variance of the policy gradient, one approach is to make use of causality by observing that the policy cannot influence past rewards. This results in a revised objective where the total rewards only include those acquired after the policy is evaluated. These rewards are considered a sample estimation of the Q function and are known as the \"reward-to-go\".\n",
        "\n",
        "## Overview\n",
        "\n",
        "The reward-to-go is defined as:\n",
        "\n",
        "$$ R_t = \\sum_{t'=t}^{T-1} \\gamma^{t'-t} r(s_{t'}, a_{t'}) $$\n",
        "\n",
        "\n",
        "The updated policy gradient will look like this:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N}  \\sum_{t=0}^{T-1} \\nabla_\\theta log \\pi_\\theta(a_{it}|s_{it}) R_t $$\n",
        "\n",
        "Thus, the updated loss function will be:\n",
        "\n",
        "$$ L(\\theta) = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T-1} log \\pi_\\theta(a_{it}|s_{it}) R_t$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Modify the `compute_loss` method to allow for the use of reward-to-go instead of the total discounted reward.\n",
        "\n",
        "Now that you have implemented REINFORCE with reward-to-go, it is time to train and evaluate a policy. Below, we provide training hyperparameters which you should use in your experiments. See the writeup for additional instructions on what metrics and figures you need to include in your submission.\n",
        "\n",
        "* Expected Training Time (Colab CPU): 10 minutes\n",
        "* Expected Reward (when calling `evaluate(100)`): 200-400"
      ],
      "metadata": {
        "id": "TtRRbGf_0R5A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Feel free to use the space below to run experiments and create plots used in your writeup.\n",
        "reseed(seed)\n",
        "env.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "env.observation_space.seed(seed)\n",
        "env.reset()\n",
        "\n",
        "policy_net_rtg = PolicyNet(env.observation_space.shape[0], env.action_space.n, 128)\n",
        "\n",
        "reinforce_rtg = PolicyGradient(env, policy_net_rtg, seed, reward_to_go=True)\n",
        "reinforce_rtg.train(num_iterations=200, batch_size=10, gamma=0.99, lr=0.001)\n",
        "\n",
        "visualize(algorithm=reinforce_rtg, video_name=\"reinforce_rtg\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "6QlGoFBW07pX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d160963-957a-4dc9-863e-7ef1f412aaca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Advantage Actor-Critic (Extra Credit)\n",
        "\n",
        "In this part of the assignment, we will implement a version of the Advantage Actor-Critic (A2C) algorithm. This algorithm is an extension to the REINFORCE algorithm, and it combines both value-based and policy-based methods. **See MACRL 11.8 for more information on Actor-Critic methods.**\n",
        "\n",
        "## Overview\n",
        "\n",
        "The A2C algorithm reduces the variance of the model by subtracting a baseline from the sum of rewards. This modifies our original objective function as follows:\n",
        "\n",
        "$$ J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[r(\\tau) - b] $$\n",
        "\n",
        "where $b$ is the baseline.\n",
        "\n",
        "In A2C, there is both a policy network $\\pi_{\\theta}$ (actor network) and a value network $V_ϕ$ (critic network). The policy network is used to select actions, while the value network is used to estimate the value of a state. The value network will be used to compute the baseline $b$ in A2C.\n",
        "\n",
        "Specifically, using $V_{\\phi}(s_t)$ as the baseline, and using (discounted) reward-to-go in place of total discounted reward (as done in part 2), A2C approximates $ \\nabla_\\theta J(\\theta)$ as follows:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N}  \\sum_{t=0}^{T-1} \\nabla_\\theta log \\pi_\\theta(a_{it}|s_{it}) \\bigl(R_t - V_ϕ (s_{it}) \\bigl) $$\n",
        "\n",
        "This algorithm is called **Advantage** Actor-Critic because the quantity $R_t - V_ϕ (s_{t})$ is an approximation of what is known as the *advantage function* $A^{\\pi_{\\theta}}$:\n",
        "\n",
        "$$A^{\\pi_{\\theta}}(s_t,a_t) = Q^{\\pi_{\\theta}}(s_t,a_t) - V^{\\pi_{\\theta}}(s_t) \\approx R_t - V_ϕ (s_t)$$\n",
        "\n",
        "where $Q^{\\pi_{\\theta}}(s_t,a_t)$ and $V^{\\pi_{\\theta}}(s_t)$ are the actual exact Q and value functions for ${\\pi_{\\theta}}$.\n",
        "\n",
        "Defining $\\tilde{A_t} = R_t - V_ϕ (s_t)$ to be the approximate advantage function, we have:\n",
        "\n",
        "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N}  \\sum_{t=0}^{T-1} \\nabla_\\theta log \\pi_\\theta(a_{it}|s_{it}) \\tilde{A_t}$$\n",
        "\n",
        "For A2C, we have two loss functions: one for the policy network and one for the value network. The policy loss will be defined similarly to the loss in reward-to-go REINFORCE, replacing $R_t$ with $\\tilde{A_t}$ and additionally taking the average loss within each episode rather than using total loss (as is commonly done in practice):\n",
        "\n",
        "$$ L_{\\text{policy}} = - \\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{T}\\sum_{t=0}^{T-1} \\log \\pi_{\\theta}(a_{it}|s_{it}) \\tilde{A_{it}} $$\n",
        "\n",
        "The value loss will be the average (across episodes) mean squared error between the estimated value of the state and the reward to go. But note that we already defined the difference between reward to go and estimated value as the approximate advantage function. So we have:\n",
        "\n",
        "$$ L_{\\text{value}} = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{T}\\sum_{t=0}^{T} (V_{\\phi}(s_{it}) - R_{it})^2 = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{1}{T}\\sum_{t=0}^{T} \\tilde{A_{it}}^2$$\n",
        "\n",
        "## Instructions\n",
        "\n",
        "You will need to implement the following:\n",
        "\n",
        "1. `ValueNet` class - This class will define the value network used in the A2C algorithm.\n",
        "\n",
        "2. `ActorCriticPolicyGradient` class - This class will define the A2C algorithm. It will be a modified version of the `PolicyGradient` class that includes the value network and the A2C loss functions.\n",
        "\n",
        "Follow the instructions below to implement each of these components.\n",
        "\n",
        "### `ValueNet` class\n",
        "\n",
        "The `ValueNet` class should define a neural network that takes in a state and outputs an estimate of the value of that state. The network should have the following architecture:\n",
        "\n",
        "- Input layer: a fully-connected layer with `state_dim` input nodes and `hidden_dim` output nodes, followed by a ReLU activation function.\n",
        "\n",
        "- Output layer: a fully-connected layer with `hidden_dim` input nodes and 1 output node.\n",
        "\n",
        "You should use the `nn` module of PyTorch to define this network."
      ],
      "metadata": {
        "id": "UaLnEgl60-VC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, state_dim: int, hidden_dim: int):\n",
        "        \"\"\"Value network for the Actor-Critic algorithm.\n",
        "\n",
        "        Args:\n",
        "            state_dim (int): Dimension of the state space.\n",
        "            hidden_dim (int): Dimension of the hidden layers.\n",
        "        \"\"\"\n",
        "        super(ValueNet, self).__init__()\n",
        "        # TODO: Implement the value network for the A2C algorithm here\n",
        "\n",
        "    def forward(self, state: torch.Tensor):\n",
        "        \"\"\"Forward pass of the value network.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): State of the environment.\n",
        "\n",
        "        Returns:\n",
        "            x (torch.Tensor): Estimated value of the state.\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass for the value network here\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "uj3ggq0DsFWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `ActorCriticPolicyGradient` class\n",
        "\n",
        "The `ActorCriticPolicyGradient` class should define the A2C algorithm.\n",
        "\n",
        "The `ActorCriticPolicyGradient` class should override the following methods from `PolicyGradient`:\n",
        "\n",
        "- `__init__(self, env, policy_net: PolicyNet, value_net: ValueNet, reward_to_go: bool = True)`: Constructor method that initializes the environment, policy network, and value network. You can ignore the `reward_to_go` argument here as in A2C we will always be using reward-to-go.\n",
        "\n",
        "- `compute_loss(self, episode, gamma)`: Method that computes the two loss functions for the A2C algorithm.\n",
        "\n",
        "- `update_policy(self, episodes, optimizer, gamma)`: Method that updates the policy network and value network using a batch of episodes.\n",
        "\n",
        "- `train(self, num_episodes, batch_size, gamma, lr)`: Method that trains the policy network and value network using the A2C algorithm."
      ],
      "metadata": {
        "id": "h-PjjlTKqkRm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class ActorCriticPolicyGradient(PolicyGradient):\n",
        "    def __init__(self, env, policy_net, value_net, seed, reward_to_go: bool = True):\n",
        "        \"\"\"A2C algorithm.\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): Environment\n",
        "            policy_net (PolicyNet): Policy network\n",
        "            value_net (ValueNet): Value network\n",
        "            seed (int): Seed\n",
        "            reward_to_go (bool): Not used\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.policy_net = policy_net.to(self.device)\n",
        "        self.value_net = value_net.to(self.device)\n",
        "        self.seed = seed\n",
        "        self.env.seed(self.seed)\n",
        "        self.env.action_space.seed(self.seed)\n",
        "        self.env.observation_space.seed(self.seed)\n",
        "        torch.manual_seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "    def compute_loss(self, episode, gamma):\n",
        "        \"\"\"Compute the loss function for the A2C algorithm\n",
        "\n",
        "        Args:\n",
        "            episode (list): List of tuples (state, action, reward)\n",
        "\n",
        "        Returns:\n",
        "            policy_loss (torch.Tensor): Value of policy loss function\n",
        "            value_loss (torch.Tensor): Value of value loss function\n",
        "        \"\"\"\n",
        "        # TODO: Compute the policy loss and value loss for A2C\n",
        "        policy_loss = None\n",
        "        value_loss = None\n",
        "        return policy_loss, value_loss\n",
        "\n",
        "    def update_policy(self, episodes, optimizer, value_optimizer, gamma):\n",
        "        \"\"\"Update the policy network and value network using the batch of episodes\n",
        "\n",
        "        Args:\n",
        "            episodes (list): List of episodes\n",
        "            optimizer (torch.optim): Optimizer for policy network\n",
        "            value_optimizer (torch.optim): Optimizer for value network\n",
        "            gamma (float): Discount factor\n",
        "        \"\"\"\n",
        "        # TODO: Compute the policy and value loss for each episode\n",
        "\n",
        "        # TODO: Update the policy network and value network using average loss across the batch\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, num_iterations, batch_size, gamma, lr):\n",
        "        \"\"\"Train the policy network and value network using the A2C algorithm\n",
        "\n",
        "        Args:\n",
        "            num_iterations (int): Number of iterations to train the policy and value networks\n",
        "            batch_size (int): Number of episodes per batch\n",
        "            gamma (float): Discount factor\n",
        "            lr (float): Learning rate\n",
        "        \"\"\"\n",
        "        self.policy_net.train()\n",
        "        optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=lr)\n",
        "        # TODO: Implement the training loop for the A2C algorithm here. \n",
        "        # Update the policy and value networks every iteration, and use one\n",
        "        # batch per iteration."
      ],
      "outputs": [],
      "metadata": {
        "id": "CMATH2Jpqkyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have implemented A2C, it is time to train and evaluate a policy. Below, we provide training hyperparameters which you should use in your experiments. See the writeup for additional instructions on what metrics and figures you need to include in your submission.\n",
        "\n",
        "* Expected Training Time (Colab CPU): 10 minutes\n",
        "* Expected Reward (when calling `evaluate(100)`): 400-500"
      ],
      "metadata": {
        "id": "QDjXd6fzSznq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Feel free to use the space below to run experiments and create plots used in your writeup.\n",
        "reseed(seed)\n",
        "env.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "env.observation_space.seed(seed)\n",
        "env.reset()\n",
        "\n",
        "policy_net_a2c = PolicyNet(env.observation_space.shape[0], env.action_space.n, 128)\n",
        "value_net = ValueNet(env.observation_space.shape[0], 128)\n",
        "\n",
        "a2c = ActorCriticPolicyGradient(env, policy_net_a2c, value_net, seed)\n",
        "a2c.train(num_iterations=200, batch_size=10, gamma=0.99, lr=0.01)\n",
        "\n",
        "visualize(algorithm=a2c, video_name=\"a2c\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "OheSn_AuxUiU"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.10.13 64-bit ('cs4756_a4': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "f80e6f76365c3a1e577e14ae81718f2e1aeab33a4811983ca756090a3b3aa652"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}