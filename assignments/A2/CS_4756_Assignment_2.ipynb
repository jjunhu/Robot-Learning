{"cells":[{"cell_type":"markdown","metadata":{"id":"caVTy-IeOlnM"},"source":["### **Due Date**\n","2/29/2024 at 11:59PM EST\n","\n","# **Introduction**\n","\n","Welcome to Assignment 2 of 4756. In this assignment, you will train an agent using demonstrations from an expert. Concretely, you will:\n","* Implement behavior cloning (BC) and dataset aggregation (DAgger) methods\n","* **Extra Credit:** Get imitation learning working under causal confounds\n","\n","You will use the Hopper agent for this assignment, which is part of Gym’s Mujoco Environments. Refer to the Gym website for more details about the [Hopper environment](https://gymnasium.farama.org/environments/mujoco/hopper/).\n","\n","\n","Please read through the following paragraphs carefully, as they will apply to this and all future assignments.\n","\n","**Getting Started:** This assignment should be completed in [Google Colab](https://colab.research.google.com/). In order to access the python files bc.py and dagger.py which you will be editing, it is necessary to first upload the folder A2_FILES to your google drive and then mount your Google Drive in Colab. To do so, carefully follow the directions below in the section **Mounting Google Drive to Colab**, or reference the instructions [here](https://saturncloud.io/blog/how-to-import-python-files-in-google-colaboratory/). Additionally, make sure to switch your runtime type to GPU; this will help speed up the training process.\n","\n","**Evaluation:**\n","Your code will be tested for correctness and, for certain assignments, speed. For this particular assignment, performance results will not be harshly graded (although we provide approximate expected reward numbers as lower bounds, you are not expected to replicate them exactly); however, it will be important to make an effort to justify your approach which led to the obtained results. Please remember that all assignments should be completed individually.\n","\n","**Academic Integrity:** We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else’s code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don’t try. We trust you all to submit your own work only; please don’t let us down. If you do, we will pursue the strongest consequences available to us.\n","\n","**Getting Help:** The [Resources](https://www.cs.cornell.edu/courses/cs4756/2024sp/#resources) section on the course website is your friend! If you ever feel stuck in these projects, please feel free to avail yourself to office hours and Edstem! If you are unable to make any of the office hours listed, please let TAs know and we will be happy to assist. If you need a refresher for PyTorch, please see this [60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)! For Numpy, please see the quickstart [here](https://numpy.org/doc/stable/user/quickstart.html) and full API [here](https://numpy.org/doc/stable/reference/).\n"]},{"cell_type":"markdown","metadata":{"id":"Ix2ll62jOlnP"},"source":["### **Imports**"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82558,"status":"ok","timestamp":1709318801486,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"Wr_wTb2DOlnQ","outputId":"62b29adc-1512-4d0e-9d68-f4303a861a3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","software-properties-common is already the newest version (0.99.22.9).\n","The following additional packages will be installed:\n","  libegl-dev libgl-dev libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\n","  libglvnd-dev libglx-dev libopengl-dev libosmesa6\n","The following NEW packages will be installed:\n","  libegl-dev libgl-dev libgl1-mesa-dev libgl1-mesa-glx libgles-dev libgles1 libglew-dev\n","  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libopengl-dev libosmesa6\n","  libosmesa6-dev\n","0 upgraded, 15 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 4,020 kB of archives.\n","After this operation, 19.4 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.2 [6,842 B]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libglew-dev amd64 2.2.0-4 [287 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6 amd64 23.2.1-1ubuntu3.1~22.04.2 [3,121 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6-dev amd64 23.2.1-1ubuntu3.1~22.04.2 [8,984 B]\n","Fetched 4,020 kB in 1s (5,222 kB/s)\n","Selecting previously unselected package libglx-dev:amd64.\n","(Reading database ... 121749 files and directories currently installed.)\n","Preparing to unpack .../00-libglx-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglx-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl-dev:amd64.\n","Preparing to unpack .../01-libgl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libegl-dev:amd64.\n","Preparing to unpack .../02-libegl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libegl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-glx:amd64.\n","Preparing to unpack .../03-libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n","Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Selecting previously unselected package libgles1:amd64.\n","Preparing to unpack .../04-libgles1_1.4.0-1_amd64.deb ...\n","Unpacking libgles1:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgles-dev:amd64.\n","Preparing to unpack .../05-libgles-dev_1.4.0-1_amd64.deb ...\n","Unpacking libgles-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-core-dev:amd64.\n","Preparing to unpack .../06-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libopengl-dev:amd64.\n","Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n","Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libglvnd-dev:amd64.\n","Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n","Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n","Selecting previously unselected package libgl1-mesa-dev:amd64.\n","Preparing to unpack .../09-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n","Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglu1-mesa-dev:amd64.\n","Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Selecting previously unselected package libglew-dev:amd64.\n","Preparing to unpack .../12-libglew-dev_2.2.0-4_amd64.deb ...\n","Unpacking libglew-dev:amd64 (2.2.0-4) ...\n","Selecting previously unselected package libosmesa6:amd64.\n","Preparing to unpack .../13-libosmesa6_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n","Unpacking libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n","Selecting previously unselected package libosmesa6-dev:amd64.\n","Preparing to unpack .../14-libosmesa6-dev_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n","Unpacking libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n","Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n","Setting up libgles1:amd64 (1.4.0-1) ...\n","Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n","Setting up libglx-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up libopengl-dev:amd64 (1.4.0-1) ...\n","Setting up libgl-dev:amd64 (1.4.0-1) ...\n","Setting up libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n","Setting up libegl-dev:amd64 (1.4.0-1) ...\n","Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n","Setting up libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n","Setting up libgles-dev:amd64 (1.4.0-1) ...\n","Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n","Setting up libglew-dev:amd64 (2.2.0-4) ...\n","Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  patchelf\n","0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 72.1 kB of archives.\n","After this operation, 186 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patchelf amd64 0.14.3-1 [72.1 kB]\n","Fetched 72.1 kB in 0s (192 kB/s)\n","Selecting previously unselected package patchelf.\n","(Reading database ... 121889 files and directories currently installed.)\n","Preparing to unpack .../patchelf_0.14.3-1_amd64.deb ...\n","Unpacking patchelf (0.14.3-1) ...\n","Setting up patchelf (0.14.3-1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n","Collecting free-mujoco-py\n","  Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Cython<0.30.0,>=0.29.24 (from free-mujoco-py)\n","  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.16.0)\n","Collecting fasteners==0.15 (from free-mujoco-py)\n","  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n","Collecting glfw<2.0.0,>=1.4.0 (from free-mujoco-py)\n","  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (2.31.6)\n","Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.25.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.16.0)\n","Collecting monotonic>=0.1 (from fasteners==0.15->free-mujoco-py)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n","Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (9.4.0)\n","Installing collected packages: monotonic, glfw, fasteners, Cython, free-mujoco-py\n","  Attempting uninstall: Cython\n","    Found existing installation: Cython 3.0.8\n","    Uninstalling Cython-3.0.8:\n","      Successfully uninstalled Cython-3.0.8\n","Successfully installed Cython-0.29.37 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 monotonic-1.6\n","Collecting imageio==2.4.1\n","  Downloading imageio-2.4.1.tar.gz (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.1) (1.25.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.1) (9.4.0)\n","Building wheels for collected packages: imageio\n","  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imageio: filename=imageio-2.4.1-py3-none-any.whl size=3303885 sha256=3383ac0332ad75f0bbca31156a3812fce892405ce5608a0d2cd7c0b1300cea89\n","  Stored in directory: /root/.cache/pip/wheels/96/5d/ce/bdbdb04744dac03906336eb0d01ff1e222061d3419c55c55f9\n","Successfully built imageio\n","Installing collected packages: imageio\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.31.6\n","    Uninstalling imageio-2.31.6:\n","      Successfully uninstalled imageio-2.31.6\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","free-mujoco-py 2.1.6 requires imageio<3.0.0,>=2.9.0, but you have imageio 2.4.1 which is incompatible.\n","moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= \"3.4\", but you have imageio 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed imageio-2.4.1\n","Collecting colabgymrender\n","  Downloading colabgymrender-1.1.0.tar.gz (3.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from colabgymrender) (1.0.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (4.4.2)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (4.66.2)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (2.31.0)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (0.1.10)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (1.25.2)\n","Collecting imageio<3.0,>=2.5 (from moviepy->colabgymrender)\n","  Downloading imageio-2.34.0-py3-none-any.whl (313 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender) (0.4.9)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy->colabgymrender) (9.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy->colabgymrender) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2024.2.2)\n","Building wheels for collected packages: colabgymrender\n","  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for colabgymrender: filename=colabgymrender-1.1.0-py3-none-any.whl size=3113 sha256=4d7c5cd6109d1f1ac91821de66ba5f01e32e5bef2c62ea9ea759d850796bd2a6\n","  Stored in directory: /root/.cache/pip/wheels/13/62/63/7b3acfb684dd3d665d7fc1d213427b136205a222389767e295\n","Successfully built colabgymrender\n","Installing collected packages: imageio, colabgymrender\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.4.1\n","    Uninstalling imageio-2.4.1:\n","      Successfully uninstalled imageio-2.4.1\n","Successfully installed colabgymrender-1.1.0 imageio-2.34.0\n","Collecting mujoco\n","  Downloading mujoco-3.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.4.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.7.0)\n","Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.25.2)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco) (3.1.7)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (2023.6.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (6.1.2)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (4.10.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (3.17.0)\n","Installing collected packages: mujoco\n","Successfully installed mujoco-3.1.2\n"]}],"source":["!apt-get install -y \\\n","    libgl1-mesa-dev \\\n","    libgl1-mesa-glx \\\n","    libglew-dev \\\n","    libosmesa6-dev \\\n","    software-properties-common\n","\n","!apt-get install -y patchelf\n","!pip install gym\n","\n","!pip install free-mujoco-py\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install imageio==2.4.1\n","!pip install -U colabgymrender\n","!pip install mujoco"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7522,"status":"ok","timestamp":1709318809006,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"mDT0P3a5OlnQ"},"outputs":[],"source":["import gym\n","import torch.nn as nn\n","import torch\n","import numpy as np\n","import random\n","import tqdm\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.optim import optimizer\n","import matplotlib.pyplot as plt\n","from IPython import display as ipythondisplay"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1709318809006,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"z8ZJJP4ivZl_","outputId":"2989b66a-a242-461c-d92d-ae8804b015d6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["# Setting the seed to ensure reproducability\n","def reseed(seed):\n","  torch.manual_seed(seed)\n","  random.seed(seed)\n","  np.random.seed(seed)\n","\n","reseed(42)\n"]},{"cell_type":"markdown","metadata":{"id":"xdqs6TQEOlnR"},"source":["### **Mounting Google Drive in Colab**\n","\n","Before you complete this step, make sure that you have uploaded the folder A2_FILES to your Google Drive. Once you have done that, you need to mount your Google Drive in Colab. In order to do so, run the cell below. Running this cell will prompt you to authorize Colab to access your drive. Follow the instructions to complete the authorization process."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2485,"status":"ok","timestamp":1709318811479,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"pB8NMNhXOlnR","outputId":"a7c29e8d-3b7b-4947-e5ea-71fbace83209"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"]},{"cell_type":"markdown","metadata":{"id":"x2a5e9TgOlnR"},"source":["Next, locate A2_FILES on the left panel in Colab. To do so, navigate to Files/drive/MyDrive. At this point, you should see the contents of your Google Drive. Locate A2_FILES in your drive, and if necessary, modify the cell below such that you are correctly indicating the file path to A2_FILES. You will append the path to A2_FILES to the system path. If you have completed this step correctly, you should be able to successfully import the BC and DAgger modules into this notebook."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709318811479,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"knBKVIwKOlnR"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/A2/A2_FILES')"]},{"cell_type":"markdown","metadata":{"id":"srFKEtW3OlnS"},"source":["### **Setting Up the Environment**"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112644,"status":"ok","timestamp":1709318924121,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"Z1HBxcIKOlnS","outputId":"da7a434c-d0d4-41e3-8f6f-0443a7956fd4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Compiling /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx because it changed.\n","[1/1] Cythonizing /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx\n"]},{"name":"stderr","output_type":"stream","text":["INFO:root:running build_ext\n","INFO:root:building 'mujoco_py.cymj' extension\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl\n","INFO:root:x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o -fopenmp -w\n","INFO:root:x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310\n","INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py\n","INFO:root:x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -L/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -L/usr/lib/x86_64-linux-gnu -Wl,--enable-new-dtags,-R/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py/cymj.cpython-310-x86_64-linux-gnu.so -fopenmp\n","<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n","/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n"]}],"source":["def make_env(env_id, seed=42, p_tremble=0.0):\n","    env = gym.make(env_id, render_mode=None) # Change render_mode = rbg_array to render\n","    env = gym.wrappers.RecordEpisodeStatistics(env)\n","    env.seed(seed)\n","    env.action_space.seed(seed)\n","    env.observation_space.seed(seed)\n","    return env\n","env = make_env('Hopper-v3')"]},{"cell_type":"markdown","metadata":{"id":"PxEY3uf3OlnS"},"source":["### **Visualizing the Hopper environment with random actions**\n","\n","We have provided functions to visualize the environment and compute rewards on the Hopper environment with random actions. Looking through this code will help you get familiarized with the environment, and set you up for the next parts in this assignment."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":513},"executionInfo":{"elapsed":211,"status":"ok","timestamp":1709318924331,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"acXcvL5fOlnS","outputId":"a1a27b7d-abb0-4033-9ccb-240ee0a27d80"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Reward using Random Actions =  36.194235803705816\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.axis('off')\n","done = False\n","visualize = False # set to false in order to disable rendering code\n","obs = env.reset()\n","total_random_reward = 0\n","i = 0\n","while not done:\n","    i += 1\n","    if i%5==0 and visualize:\n","        ipythondisplay.clear_output(wait=True)\n","        screen = env.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","    action = env.action_space.sample()\n","    obs, reward, done, info = env.step(action)\n","    total_random_reward += reward\n","    if done:\n","        break\n","print(\"Total Reward using Random Actions = \", total_random_reward)"]},{"cell_type":"markdown","metadata":{"id":"FihI62JwOlnS"},"source":["**Approximate expected reward for total reward using random actions: 27**"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":554,"status":"ok","timestamp":1709318924877,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"xoHvUiacOlnT","outputId":"64dd2e2a-dd1b-42bd-a802-941a93cf5004"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-03-01 18:48:44--  https://github.com/portal-cornell/cs4756-robot-learning-sp24/raw/main/assignments/A2/experts/hopper.pt\n","Resolving github.com (github.com)... 192.30.255.113\n","Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/portal-cornell/cs4756-robot-learning-sp24/main/assignments/A2/experts/hopper.pt [following]\n","--2024-03-01 18:48:44--  https://raw.githubusercontent.com/portal-cornell/cs4756-robot-learning-sp24/main/assignments/A2/experts/hopper.pt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 284515 (278K) [application/octet-stream]\n","Saving to: ‘hopper.pt’\n","\n","hopper.pt           100%[===================>] 277.85K  --.-KB/s    in 0.02s   \n","\n","2024-03-01 18:48:44 (12.2 MB/s) - ‘hopper.pt’ saved [284515/284515]\n","\n"]}],"source":["# Download Hopper expert policy\n","!wget https://github.com/portal-cornell/cs4756-robot-learning-sp24/raw/main/assignments/A2/experts/hopper.pt"]},{"cell_type":"markdown","metadata":{"id":"tbtKTFMbOlnT"},"source":["### **Neural Networks in PyTorch**\n","\n","We have provided some code for implementing simple neural networks (fully connected, multilayer perceptrons) in PyTorch, including the ExpertActor and Learner classes. We have also provided code for checkpointing for saving your best performing model. If you wish to learn more about how to construct and train neural networks in PyTorch, check out the tutorials on [pytorch.org](https://pytorch.org/)."]},{"cell_type":"markdown","metadata":{"id":"Kw_w17lhOlnT"},"source":["### ExpertActor Class"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1709318924877,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"khjGbIfJOlnT","outputId":"f239b527-ac26-436e-e97b-fe7ceee11722"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["LOG_STD_MAX = 2\n","LOG_STD_MIN = -5\n","\n","class ExpertActor(nn.Module):\n","    def __init__(self, env):\n","        super().__init__()\n","        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod(), 256)\n","        self.fc2 = nn.Linear(256, 256)\n","        self.fc_mean = nn.Linear(256, np.prod(env.action_space.shape))\n","        self.fc_logstd = nn.Linear(256, np.prod(env.action_space.shape))\n","        # action rescaling\n","        self.register_buffer(\n","            \"action_scale\",\n","            torch.tensor(\n","                (env.action_space.high - env.action_space.low) / 2.0,\n","                dtype=torch.float32,\n","            ).reshape(1, -1),\n","        )\n","        self.register_buffer(\n","            \"action_bias\",\n","            torch.tensor(\n","                (env.action_space.high + env.action_space.low) / 2.0,\n","                dtype=torch.float32,\n","            ).reshape(1, -1),\n","        )\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        mean = self.fc_mean(x)\n","        log_std = self.fc_logstd(x)\n","        log_std = torch.tanh(log_std)\n","        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (\n","            log_std + 1\n","        )\n","\n","        return mean, log_std\n","\n","    def get_action(self, x):\n","        mean, log_std = self(x)\n","        std = log_std.exp()\n","        normal = torch.distributions.Normal(mean, std)\n","        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n","        y_t = torch.tanh(x_t)\n","        action = y_t * self.action_scale + self.action_bias\n","        log_prob = normal.log_prob(x_t)\n","        # Enforcing Action Bound\n","        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n","        log_prob = log_prob.sum(1, keepdim=True)\n","        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n","        return action, log_prob, mean\n","\n","    def get_expert_action(self, obs, random_prob=0.0):\n","        if np.random.random() < random_prob:\n","            return env.action_space.sample()\n","        else:\n","            action = self.get_action(torch.tensor([obs]).float())\n","            return np.array(action[0][0].detach().cpu())\n","\n","ckpt_path = \"hopper.pt\"\n","expert = ExpertActor(env).to('cpu')\n","expert.load_state_dict(torch.load(str(ckpt_path), map_location='cpu'))"]},{"cell_type":"markdown","metadata":{"id":"jR2vvUHrOlnT"},"source":["### Learner Class"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1709318924877,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"XiZA33etOlnT"},"outputs":[],"source":["class Learner(nn.Module):\n","    def __init__(self, env, hidden_dim = 256, random_prob=0.0):\n","        super().__init__()\n","        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod(), hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc_out = nn.Linear(hidden_dim, np.prod(env.action_space.shape))\n","\n","        self.env = env\n","        self.random_prob = random_prob\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        out = F.tanh(self.fc_out(x))\n","        return out\n","\n","    def get_action(self, obs):\n","        if np.random.random() < self.random_prob:\n","            return self.env.action_space.sample()\n","        action = self.forward(torch.tensor([obs]).float().to(device))\n","        return np.array(action[0].detach().cpu())"]},{"cell_type":"markdown","metadata":{"id":"n1t7PJjCUlr7"},"source":["### Checkpointing Functions"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1709318924877,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"eJe4atJ1UkFU"},"outputs":[],"source":["def get_checkpoint_path(algo):\n","    \"\"\"Return the path to save the best performing model checkpoint.\n","\n","    Parameters:\n","        algo (str)\n","          Indicates which algorithm will be used to train the model\n","\n","    Returns:\n","        checkpoint_path (str)\n","            The path to save the best performing model checkpoint\n","    \"\"\"\n","    if algo == \"bc\":\n","      return 'best_bc_checkpoint.pth'\n","    elif algo == \"dagger\":\n","      return 'best_dagger_checkpoint.pth'\n","    return 'best_model_checkpoint.pth'\n","\n","def load_model_checkpoint(checkpoint_path):\n","    \"\"\"Load a model checkpoint from disk.\n","\n","    Parameters:\n","        checkpoint_path (str)\n","            The path to load the checkpoint from\n","\n","    Returns:\n","        model (torch.nn.Module)\n","            The model loaded from the checkpoint\n","    \"\"\"\n","    model = Learner(env)\n","    model.load_state_dict(torch.load(checkpoint_path))\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"yru8Y-CFOlnU"},"source":["### **Visualizing the Hopper environment with the expert policy**\n","\n","We have provided a visualization for computing rewards using the expert policy on the Hopper environment."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1873,"status":"ok","timestamp":1709318926749,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"WpATVTRdOlnU","outputId":"ecd0c6b1-ae8e-40b7-8239-a9f39d90c314"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.417022004702574\n","0.00011437481734488664\n","0.14675589081711304\n","0.1862602113776709\n","0.39676747423066994\n","0.4191945144032948\n","0.20445224973151743\n","0.027387593197926163\n","0.41730480236712697\n","0.14038693859523377\n","0.8007445686755367\n","0.31342417815924284\n","0.8763891522960383\n","0.08504421136977791\n","0.1698304195645689\n","0.0983468338330501\n","0.9578895301505019\n","0.6918771139504734\n","0.6865009276815837\n","0.018288277344191806\n","0.9888610889064947\n","0.2804439920644052\n","0.10322600657764203\n","0.9085955030930956\n","0.28777533858634874\n","0.019366957870297075\n","0.21162811600005904\n","0.4915731592803383\n","0.5741176054920131\n","0.5893055369032842\n","0.10233442882782584\n","0.6944001577277451\n","0.04995345894608716\n","0.6637946452197888\n","0.9445947559908133\n","0.9034019152878835\n","0.13927634725075855\n","0.3976768369855336\n","0.9275085803960339\n","0.7508121031361555\n","0.8833060912058098\n","0.7509424340273372\n","0.2699278917650261\n","0.4280911898712949\n","0.6634414978184481\n","0.11474597295337519\n","0.4499121334799405\n","0.40813680276128117\n","0.9033795205622538\n","0.00287032703115897\n","0.32664490177209615\n","0.8859420993107745\n","0.9085351509197992\n","0.015821242846556283\n","0.690896917516924\n","0.17234050834532855\n","0.9325954630371636\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-10-33d12b820a6d>:58: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  action = self.get_action(torch.tensor([obs]).float())\n"]},{"name":"stdout","output_type":"stream","text":["0.06600017272206249\n","0.7538761884612464\n","0.7115247586284718\n","0.01988013383979559\n","0.028306488020794607\n","0.860027948682888\n","0.5528219786857659\n","0.12417331511991114\n","0.5857592714582879\n","0.56103021925571\n","0.8006326726806163\n","0.8071051956187791\n","0.8635418545594287\n","0.5562402339904189\n","0.05991768951221166\n","0.044551878544761725\n","0.2257093386078547\n","0.5597169820541424\n","0.07197427968948678\n","0.5681004619199421\n","0.2523257445703234\n","0.1954294811093188\n","0.9700199890883123\n","0.23984775914758616\n","0.6199557183813798\n","0.15679139464608427\n","0.07002214371922233\n","0.6063294616533303\n","0.31736240932216075\n","0.5797452192457969\n","0.5509482191178968\n","0.6692328934531846\n","0.06633483442844157\n","0.6297175070215645\n","0.7527555537388139\n","0.26031509857854096\n","0.19343428262332774\n","0.5246703091237337\n","0.263296770487111\n","0.7350659632886695\n","0.907815852503524\n","0.013951572975597015\n","0.6167783570016576\n","0.9501761192470797\n","0.9156063497662745\n","0.39000771414124624\n","0.6043104829199732\n","0.9261814267064536\n","0.3948756129235549\n","0.17395566668046436\n","0.13507915804673132\n","0.021524805274197978\n","0.8271154711707325\n","0.17619625557505525\n","0.1309968448109169\n","0.34473665268329345\n","0.58201417994708\n","0.8447344453922219\n","0.45988026581680697\n","0.7986035911520394\n","0.4902535226199277\n","0.015533275550835723\n","0.4336763489894583\n","0.31524480309537295\n","0.5778572152845355\n","0.7879292338255043\n","0.053909272073752956\n","0.6790688365654296\n","0.00040202489135743313\n","0.37658031474577447\n","0.604716100974053\n","0.5747115047081023\n","0.28557628169569393\n","0.7500217637026599\n","0.7550821884676802\n","0.8644794300545998\n","0.6707887907875872\n","0.3821027520315172\n","0.4014795834695406\n","0.6219193679203014\n","0.9738020779272523\n","0.19856988842711087\n","0.343346239774423\n","0.8799982885634318\n","0.6627198123752622\n","0.25236670150458973\n","0.5277146463087466\n","0.5724885171916063\n","0.5190116274640558\n","0.5688579907047155\n","0.3426889079532818\n","0.37792417932809985\n","0.982817113730445\n","0.8118586977205398\n","0.6884132523859433\n","0.16097143681560877\n","0.345172051155215\n","0.5925118687657968\n","0.9163055534683507\n","0.2571182937821962\n","0.19296273201911285\n","0.7285856679745962\n","0.24803355837723073\n","0.41584871826752734\n","0.23366613923925006\n","0.5158570169685298\n","0.15267164409316325\n","0.5440101188139381\n","0.1445455401246598\n","0.2220491397999227\n","0.7852960282216189\n","0.3243624597261865\n","0.8447096076020696\n","0.866608274166513\n","0.8264069976293413\n","0.09874340182034835\n","0.703516988152653\n","0.799615261736028\n","0.7702387345549799\n","0.2596983932983531\n","0.6323033174301278\n","0.7965886780072875\n","0.7827494147841719\n","0.30024833953390007\n","0.9013084363492738\n","0.9747403708550822\n","0.9939130246104789\n","0.5264259339055213\n","0.3557051709838811\n","0.1603951795196491\n","0.03039968992878883\n","0.8623462528380441\n","0.6909421420661686\n","0.44190428074842336\n","0.9897517076637052\n","0.24773290176235485\n","0.7501724132979928\n","0.05692943841109499\n","0.21196016464367595\n","0.29733138150727223\n","0.593432449464621\n","0.3810161240622142\n","0.5111414782971282\n","0.9594343210617524\n","0.03232306662357398\n","0.46500148156350607\n","0.2214327343234883\n","0.081473964877262\n","0.10901876401906829\n","0.8029632373737379\n","0.7662113810757705\n","0.8458514830276777\n","0.8240098704123188\n","0.1434230486702449\n","0.01833264291300729\n","0.4585838137084318\n","0.027783348848669953\n","0.3948504804606394\n","0.45240482674645155\n","0.47807250671783097\n","0.8031633421141696\n","0.904686160343623\n","0.7738743427204187\n","0.6185135669120855\n","0.538627282576347\n","0.9511937854902045\n","0.7959669419422694\n","0.14555823115839706\n","0.187631672902434\n","0.9058094960083019\n","0.7111224587658159\n","0.9092932045442544\n","0.24985068022503676\n","0.119457050333729\n","0.14679237341724316\n","0.8190891786335308\n","0.9824174492929428\n","0.5336533449712114\n","0.9107728311738374\n","0.43359232727316455\n","0.9388864773697133\n","0.7168786636316892\n","0.027287223517468817\n","0.3259898117700253\n","0.5585165509225178\n","0.45285349995153745\n","0.2900968516874258\n","0.5767559349870908\n","0.5172675985011298\n","0.426474788501396\n","0.37129376127905445\n","0.9368683813090937\n","0.9202065143056605\n","0.08748220955359087\n","0.31437661611405043\n","0.6070941619724605\n","0.8163515118272141\n","0.7018765297563104\n","0.5742190888408597\n","0.05696439948283272\n","0.6641025553547935\n","0.5190159840736788\n","0.5707158515462564\n","0.8168351087916219\n","0.9759890683309049\n","0.5956079336026455\n","0.09357709581748075\n","0.4517331486914753\n","0.9753500342702865\n","0.9727875919326552\n","0.8242378396244978\n","0.668732770206732\n","0.013136356923072245\n","0.49207180149355845\n","0.4686283427461092\n","0.13766274094560316\n","0.7582782604769962\n","0.9843834501935107\n","0.33870802548167966\n","0.7548914570605327\n","0.12482254311167629\n","0.5045191697009946\n","0.7701499563120922\n","0.022915131322381765\n","0.8099887106624163\n","0.6724698446537339\n","0.44924675358663757\n","0.6443610847569929\n","0.4844284455143756\n","0.8303995700373394\n","0.6736984238986317\n","0.27411977516685215\n","0.6717297848174203\n","0.8558283672539277\n","0.7473208103319232\n","0.773799287244182\n","0.8076984091122095\n","0.21369323762958115\n","0.3086419476306511\n","0.7444731530777321\n","0.21411213682064845\n","0.14251833776584466\n","0.026627884691621473\n","0.6745640237634105\n","0.0805295271283859\n","0.20762566180381437\n","0.7113145181364867\n","0.30451798792062024\n","0.43530595762951996\n","0.7060518043359332\n","0.1262101037493587\n","0.15983365034409336\n","0.43118175516037094\n","0.14675148076429412\n","0.18874506687718595\n","0.7543059505762987\n","0.60095424810764\n","0.6382187106203334\n","0.29548228565549706\n","0.9453084399999497\n","0.7821818168314053\n","0.8352716024899659\n","0.3950968688899732\n","0.08010364380604929\n","0.37015417755373825\n","0.49411626589632507\n","0.20645405888943302\n","0.5079216999452875\n","0.3570616088827919\n","0.7875518396525072\n","0.9857088237131878\n","0.5724051124346493\n","0.7871162896558341\n","0.5279039784081266\n","0.14993148491725594\n","0.2166172083287794\n","0.7229151935562137\n","0.8619665576670734\n","0.8602369974231152\n","0.4032204721058492\n","0.7169290016924699\n","0.2780850486129828\n","0.9339026083148878\n","0.7288508775192559\n","0.706956245248761\n","0.3748759435368594\n","0.7506243191751754\n","0.4018659239678374\n","0.0031128578251921057\n","0.896416602456847\n","0.12076718430922972\n","0.3020967312900983\n","0.5431664302380945\n","0.13835468979294652\n","0.6138710897412526\n","0.45736018169099846\n","0.8281353607315121\n","0.3456988211755023\n","0.22148205573352875\n","0.31476568732401644\n","0.8773604746948126\n","0.7844574187919231\n","0.6562293317171314\n","0.43298150696764814\n","0.6054790010309347\n","0.5047006080958943\n","0.8428998401928499\n","0.5732722715496846\n","0.5178600481723219\n","0.8479393924434762\n","0.700726036482538\n","0.9488362882802196\n","0.8379779620477293\n","0.989340056166638\n","0.3221295136147696\n","0.008939102371245067\n","0.8612116383667944\n","0.2557451938059312\n","0.47786200443222115\n","0.9278489928250108\n","0.49154515001693533\n","0.41857802576409864\n","0.39799063940792245\n","0.1895517028339273\n","0.2942156882442547\n","0.1443154002290481\n","0.7159456994623945\n","0.7945783511847522\n","0.7918210395041538\n","0.7778484768591754\n","0.6477706331874812\n","0.321819963387904\n","0.4086372250132624\n","0.4069219941293579\n","0.32031932582203126\n","0.6363061258559836\n","0.8574844990835966\n","0.25203308013083936\n","0.43293850873416306\n","0.33027693652508094\n","0.26865012390622656\n","0.2952887943998478\n","0.4879214918052024\n","0.8883864343953859\n","0.585348458938251\n","0.4461172190344297\n","0.2789908904569709\n","0.6824537097668154\n","0.013767512557716799\n","0.938481891856734\n","0.7797442954825999\n","0.34195283630245243\n","0.716770814146376\n","0.6884973179489188\n","0.6923601216234089\n","0.4246491159313397\n","0.3553079116201222\n","0.6316466268659396\n","0.6135886945028356\n","0.16994071421725776\n","0.5141750435258828\n","0.18395344154722204\n","0.42893233402990294\n","0.16151077048601192\n","0.2618804041035202\n","0.8003322457764017\n","0.6070154607890389\n","0.5096132942175586\n","0.8596509535692011\n","0.633474012622421\n","0.4705878794988857\n","0.9482991740726651\n","0.15172487103197363\n","0.5656621125439275\n","0.42803746946827015\n","0.8499698885159654\n","0.579360540297233\n","0.06473998344154885\n","0.05299454266798953\n","0.42275266164456093\n","0.6236704109977018\n","0.28462388100694214\n","0.7035193311050811\n","0.3785805932506022\n","0.7470048290299451\n","0.7953011674056428\n","0.5256694443509794\n","0.6442320198790318\n","0.2292050272920657\n","0.3824674572709176\n","0.9794833541798064\n","0.7744101333360395\n","0.8891310891074828\n","0.24551868018650747\n","0.04353426375776537\n","0.5564069156709253\n","0.9950523223391111\n","0.5167063597676121\n","0.5711757391862334\n","0.6309592003395305\n","0.8749025512123809\n","0.7084608700517172\n","0.4948430841825636\n","0.1507843983721926\n","0.14222125307278666\n","0.4770129944022632\n","0.8859979711653513\n","0.4090908319168599\n","0.07201203516976995\n","0.025753482406583372\n","0.5035095098483965\n","0.10938291419358459\n","0.49993237109919997\n","0.14360699787420927\n","0.3992186053524538\n","0.1916757389098901\n","0.2902979970618682\n","0.016715589077006432\n","0.38108149129345237\n","0.07091840047038722\n","0.016575882900934147\n","0.6517892760100904\n","0.32102631416731586\n","0.9934604630998499\n","0.6996231823995839\n","0.03972870668308481\n","0.4740062899880546\n","0.9372520621284489\n","0.5396491100933506\n","0.4466350458236905\n","0.25358174937162836\n","0.32836139008759013\n","0.22012867402698122\n","0.1427932828030004\n","0.8701917462053776\n","0.3860040110225955\n","0.52580196995147\n","0.8741259355980283\n","0.8125073041746718\n","0.5278467961179595\n","0.5546673107876667\n","0.3117029177189814\n","0.3259672073089859\n","0.3258099666132048\n","0.7517077209192669\n","0.46947902855992574\n","0.041475079576498874\n","0.03711266449399442\n","0.6703500319361684\n","0.767788977658173\n","0.03985992663545257\n","0.19341639901491836\n","0.05231294643747675\n","0.5120610275239188\n","0.43235559476434404\n","0.4540590560591513\n","0.8730681484470995\n","0.823003040648471\n","0.050912383516887205\n","0.0633434434563398\n","0.863829145501452\n","0.4731119528858785\n","0.7728856021671178\n","0.04329093818615859\n","0.23941104413775594\n","0.9438926277837961\n","0.9734873973251963\n","0.8978507176676818\n","0.23581463714751372\n","0.6842180509832275\n","0.8704248568230661\n","0.6049268729742817\n","0.2534390424997808\n","0.8146191812616553\n","0.13051067133926597\n","0.6183458693278855\n","0.2482907289970986\n","0.8726861649194619\n","0.0644302555796068\n","0.20338083282405106\n","0.8817706230828073\n","0.5984646931046363\n","0.624821294185499\n","0.2821394484048606\n","0.5676903301191493\n","0.22700116521465796\n","0.23944623690468037\n","0.16184771412383514\n","0.6021836538927108\n","0.34567921329602547\n","0.5989852889734197\n","0.05917720718572239\n","0.9482094981876252\n","0.1925560209550684\n","0.0073189693278638\n","0.9176063476443932\n","0.8551902839041353\n","0.822578152523871\n","0.3209863745244532\n","0.7213033441865708\n","0.1267357091420086\n","0.03606831456985993\n","0.7883011896679689\n","0.3033456717085691\n","0.9974849927283295\n","0.29337627848605663\n","0.5071194235482045\n","0.5878715062961865\n","0.3052882993474333\n","0.24444799937161088\n","0.6958461339323264\n","0.9710833196254638\n","0.8061631629534083\n","0.0800609772712173\n","0.44567295640165605\n","0.4487372588243219\n","0.6816453376026229\n","0.6186007425615364\n","0.760802456305756\n","0.9041029859138049\n","0.17667889307234141\n","0.3205646539951602\n","0.6135258996684316\n","0.4276650996969642\n","0.11503501205024624\n","0.19384776750950716\n","0.5996360084237168\n","0.4911099613042962\n","0.23507763668617032\n","0.19859150305392625\n","0.7293976982561526\n","0.33039174636170277\n","0.42844247595107343\n","0.6570654246228895\n","0.200727488035091\n","0.8944324184275724\n","0.8709456091610533\n","0.32984456268188234\n","0.8992209495659688\n","0.8785878632047704\n","0.8497689100179306\n","0.34433919545005387\n","0.6353709614884534\n","0.8218493747640105\n","0.9563015097552321\n","0.1978327846479193\n","0.3367459987011673\n","0.38023649611726473\n","0.5188042507334142\n","0.07457407125147986\n","0.12341885981472989\n","0.4139198457294089\n","0.9299556236533179\n","0.3948822625368005\n","0.5609487551219452\n","0.48719390907342375\n","0.26199970471933565\n","0.022271511532804156\n","0.18846839966641848\n","0.35520938933964374\n","0.12277212944941995\n","0.8894424754644789\n","0.3092463846587796\n","0.22759773832225305\n","0.2034338243700906\n","0.8730943782696405\n","0.47049767615220905\n","0.49582639708521237\n","0.27560310276085875\n","0.38698923425629217\n","0.2544381023144221\n","0.38326345680528673\n","0.7175949621947544\n","0.11819262934703845\n","0.7441359213695352\n","0.24289157496487068\n","0.638355320741173\n","0.06748934306754228\n","0.4599567310742584\n","0.2538886209427397\n","0.4287951603487977\n","0.12671338384692277\n","0.07934158295831895\n","0.7417048002315554\n","0.6724110989339258\n","0.6783769722513199\n","0.23090845048417918\n","0.18498296555082983\n","0.9825897893050853\n","0.3473694203728128\n","0.5464545907014307\n","0.48372695118890585\n","0.7872634475242615\n","0.5370071168405407\n","0.6043068436688722\n","0.39282294499696246\n","0.7906369935895988\n","0.3938840078553765\n","0.404001609054379\n","0.45064326867213444\n","0.2179689300611679\n","0.6896393990795577\n","0.746657472123005\n","0.7005330726195026\n","0.8626166945052219\n","0.2631390239238742\n","0.978672805158347\n","0.6605983559285987\n","0.21077094208701874\n","0.10495366832992803\n","0.5379877201396809\n","0.5559002111265755\n","0.3906104987369058\n","0.6784824539979241\n","0.5614912587993742\n","0.6131123353670123\n","0.5968885249459731\n","0.4410066204088712\n","0.03760387864960013\n","0.5182815803268124\n","0.2772001047825985\n","0.7679628782813116\n","0.5490534438505197\n","0.6495830548784789\n","0.07473144501476114\n","0.9192375042561725\n","0.5997403444720987\n","0.896043491575002\n","0.9985205070898059\n","0.07054204474369918\n","0.01474379089994693\n","0.09476691645722912\n","0.6851729969076996\n","0.3466948813585782\n","0.2538388823471872\n","0.725642545494869\n","0.46152125324711457\n","0.02896618617789848\n","0.7710053579546823\n","0.796558587536711\n","0.30788244381764673\n","0.9769161619458502\n","0.951935167567925\n","0.5180325615944087\n","0.7103545783196094\n","0.6052128951232129\n","0.7739162499564685\n","0.08899822082237796\n","0.5268518758918344\n","0.8006596581292397\n","0.17786474795134855\n","0.9573949263266688\n","0.037862587720430496\n","0.005797392917688993\n","0.05569841271210463\n","0.3560164860311865\n","0.47695092248677795\n","0.5030051950509548\n","0.111245672567435\n","0.8519457712872955\n","0.8905068324120895\n","0.44380519836651855\n","0.22580692243123512\n","0.28462396896873543\n","0.8905522863783111\n","0.15614956444079475\n","0.6319949504631371\n","0.3451159147800812\n","0.4443481574461272\n","0.16531849048023983\n","0.6910273415567042\n","0.8733231340367321\n","0.6296079567189131\n","0.516198461077069\n","0.9841265438849168\n","0.2528414820768151\n","0.047537594227760493\n","0.49019531224423174\n","0.278328130846902\n","0.2946920415609042\n","0.5905465222435671\n","0.7025230339614009\n","0.6332092658064775\n","0.28704394615550866\n","0.9370803847989643\n","0.9380285431563524\n","0.9331557628232255\n","0.2835445136036545\n","0.5514315736733725\n","0.4421011106267658\n","0.9473759179539593\n","0.9639177674239978\n","0.4843157730323856\n","0.14318978170876284\n","0.6618883678367976\n","0.05935722533895749\n","0.46513394351807746\n","0.25666098995297626\n","0.24017220722137722\n","0.9898834533371155\n","0.8214467402262973\n","0.05797581337276703\n","0.9351531155423887\n","0.28926049548291\n","0.8499305352314189\n","0.4680495548664728\n","0.15803183496381268\n","0.26551201747238584\n","0.6744988441622575\n","0.32164814560277766\n","0.14578543121477705\n","0.2923137620392837\n","0.08793868377060332\n","0.08818864557151007\n","0.12040087685772338\n","0.5572682300749138\n","0.8604566673368852\n","0.02715882932586222\n","0.805984932289666\n","0.6696465998755331\n","0.8974982823028202\n","0.5578253037149105\n","0.6919839045242188\n","0.5203136358633165\n","0.5057793867898664\n","0.28184508814324294\n","0.969870570344714\n","0.25979169330441454\n","0.9552622535381868\n","0.08353420706174108\n","0.22449347883595772\n","0.5681525429491051\n","0.10781390079459241\n","0.6251210683054896\n","0.08725252387198179\n","0.6721444096568486\n","0.8957291085287041\n","0.9615725424345338\n","0.4062884276883151\n","0.28798687927475153\n","0.605896361217858\n","0.8786609763158111\n","0.7661883652028217\n","0.001032932657734631\n","0.6570403290247293\n","0.18373941187600173\n","0.36436547985702794\n","0.02771174012236499\n","0.9814104657541125\n","0.41763234681418326\n","0.64990115308969\n","0.31161391346070466\n","0.6074418935145536\n","0.8460545992364693\n","Total Reward using Expert Policy = 3485.88026939468\n","Total Reward using Random Actions = 36.194235803705816\n","\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.axis('off')\n","done = False\n","visualize = False # set to false in order to disable rendering code\n","reseed(1)\n","obs = env.reset(seed=1)\n","total_expert_reward = 0\n","i = 0\n","while not done:\n","    i += 1\n","    if i%20==0 and visualize:\n","        ipythondisplay.clear_output(wait=True)\n","        screen = env.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","    with torch.no_grad():\n","        action = expert.get_expert_action(obs)\n","    obs, reward, done, info = env.step(action)\n","    total_expert_reward += reward\n","    if done:\n","        break\n","print(f\"Total Reward using Expert Policy = {total_expert_reward}\\nTotal Reward using Random Actions = {total_random_reward}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"BDhO0BKCOlnU"},"source":["**Approximate expected reward for total reward using expert policy: 2238**"]},{"cell_type":"markdown","metadata":{"id":"S-4h88CAOlnU"},"source":["### **Data collection**\n","\n","We have provided some code to collect 50 demonstrations using the expert policy. To collect a different number of  trajectories, change the value of the NUM_TRAJS variable."]},{"cell_type":"markdown","metadata":{"id":"uvy7CL5kOlnU"},"source":["### Collecting and processing offline data"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47193,"status":"ok","timestamp":1709318475925,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"xsfopEx_OlnV","outputId":"2d7aacfc-9a78-4e02-c378-71be819d3f02"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/50 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  1\n"]},{"name":"stderr","output_type":"stream","text":["\r  2%|▏         | 1/50 [00:01<01:19,  1.62s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  2\n"]},{"name":"stderr","output_type":"stream","text":["\r  4%|▍         | 2/50 [00:02<01:01,  1.29s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  3\n"]},{"name":"stderr","output_type":"stream","text":["\r  6%|▌         | 3/50 [00:03<00:50,  1.08s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  4\n"]},{"name":"stderr","output_type":"stream","text":["\r  8%|▊         | 4/50 [00:04<00:51,  1.12s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  5\n"]},{"name":"stderr","output_type":"stream","text":["\r 10%|█         | 5/50 [00:05<00:50,  1.12s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  6\n"]},{"name":"stderr","output_type":"stream","text":["\r 12%|█▏        | 6/50 [00:07<00:53,  1.22s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  7\n"]},{"name":"stderr","output_type":"stream","text":["\r 14%|█▍        | 7/50 [00:08<00:56,  1.31s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  8\n"]},{"name":"stderr","output_type":"stream","text":["\r 16%|█▌        | 8/50 [00:12<01:21,  1.93s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  9\n"]},{"name":"stderr","output_type":"stream","text":["\r 18%|█▊        | 9/50 [00:13<01:13,  1.78s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  10\n"]},{"name":"stderr","output_type":"stream","text":["\r 20%|██        | 10/50 [00:15<01:14,  1.85s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  11\n"]},{"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 11/50 [00:16<01:08,  1.75s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  12\n"]},{"name":"stderr","output_type":"stream","text":["\r 24%|██▍       | 12/50 [00:17<00:56,  1.48s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  13\n"]},{"name":"stderr","output_type":"stream","text":["\r 26%|██▌       | 13/50 [00:19<00:53,  1.43s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  14\n"]},{"name":"stderr","output_type":"stream","text":["\r 28%|██▊       | 14/50 [00:19<00:44,  1.24s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  15\n"]},{"name":"stderr","output_type":"stream","text":["\r 30%|███       | 15/50 [00:20<00:37,  1.06s/it]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  16\n"]},{"name":"stderr","output_type":"stream","text":["\r 32%|███▏      | 16/50 [00:21<00:32,  1.06it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  17\n"]},{"name":"stderr","output_type":"stream","text":["\r 34%|███▍      | 17/50 [00:22<00:29,  1.11it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  18\n"]},{"name":"stderr","output_type":"stream","text":["\r 36%|███▌      | 18/50 [00:23<00:30,  1.05it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  19\n"]},{"name":"stderr","output_type":"stream","text":["\r 38%|███▊      | 19/50 [00:24<00:29,  1.06it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  20\n"]},{"name":"stderr","output_type":"stream","text":["\r 40%|████      | 20/50 [00:24<00:27,  1.09it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  21\n"]},{"name":"stderr","output_type":"stream","text":["\r 42%|████▏     | 21/50 [00:25<00:26,  1.11it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  22\n"]},{"name":"stderr","output_type":"stream","text":["\r 44%|████▍     | 22/50 [00:26<00:25,  1.10it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  23\n"]},{"name":"stderr","output_type":"stream","text":["\r 46%|████▌     | 23/50 [00:27<00:22,  1.19it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  24\n"]},{"name":"stderr","output_type":"stream","text":["\r 48%|████▊     | 24/50 [00:28<00:20,  1.25it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  25\n"]},{"name":"stderr","output_type":"stream","text":["\r 50%|█████     | 25/50 [00:28<00:19,  1.28it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  26\n"]},{"name":"stderr","output_type":"stream","text":["\r 52%|█████▏    | 26/50 [00:29<00:16,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  27\n"]},{"name":"stderr","output_type":"stream","text":["\r 54%|█████▍    | 27/50 [00:30<00:17,  1.33it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  28\n"]},{"name":"stderr","output_type":"stream","text":["\r 56%|█████▌    | 28/50 [00:30<00:16,  1.36it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  29\n"]},{"name":"stderr","output_type":"stream","text":["\r 58%|█████▊    | 29/50 [00:31<00:16,  1.24it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  30\n"]},{"name":"stderr","output_type":"stream","text":["\r 60%|██████    | 30/50 [00:32<00:14,  1.33it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  31\n"]},{"name":"stderr","output_type":"stream","text":["\r 62%|██████▏   | 31/50 [00:33<00:13,  1.43it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  32\n"]},{"name":"stderr","output_type":"stream","text":["\r 64%|██████▍   | 32/50 [00:34<00:13,  1.31it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  33\n"]},{"name":"stderr","output_type":"stream","text":["\r 66%|██████▌   | 33/50 [00:34<00:12,  1.36it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  34\n"]},{"name":"stderr","output_type":"stream","text":["\r 68%|██████▊   | 34/50 [00:35<00:12,  1.31it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  35\n"]},{"name":"stderr","output_type":"stream","text":["\r 70%|███████   | 35/50 [00:36<00:10,  1.38it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  36\n"]},{"name":"stderr","output_type":"stream","text":["\r 72%|███████▏  | 36/50 [00:36<00:09,  1.45it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  37\n"]},{"name":"stderr","output_type":"stream","text":["\r 74%|███████▍  | 37/50 [00:37<00:09,  1.31it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  38\n"]},{"name":"stderr","output_type":"stream","text":["\r 76%|███████▌  | 38/50 [00:38<00:08,  1.34it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  39\n"]},{"name":"stderr","output_type":"stream","text":["\r 78%|███████▊  | 39/50 [00:39<00:08,  1.37it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  40\n"]},{"name":"stderr","output_type":"stream","text":["\r 80%|████████  | 40/50 [00:39<00:07,  1.28it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  41\n"]},{"name":"stderr","output_type":"stream","text":["\r 82%|████████▏ | 41/50 [00:40<00:07,  1.20it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  42\n"]},{"name":"stderr","output_type":"stream","text":["\r 84%|████████▍ | 42/50 [00:41<00:06,  1.29it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  43\n"]},{"name":"stderr","output_type":"stream","text":["\r 86%|████████▌ | 43/50 [00:42<00:04,  1.41it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  44\n"]},{"name":"stderr","output_type":"stream","text":["\r 88%|████████▊ | 44/50 [00:43<00:04,  1.31it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  45\n"]},{"name":"stderr","output_type":"stream","text":["\r 90%|█████████ | 45/50 [00:43<00:03,  1.38it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  46\n"]},{"name":"stderr","output_type":"stream","text":["\r 92%|█████████▏| 46/50 [00:44<00:02,  1.46it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  47\n"]},{"name":"stderr","output_type":"stream","text":["\r 94%|█████████▍| 47/50 [00:45<00:02,  1.39it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  48\n"]},{"name":"stderr","output_type":"stream","text":["\r 96%|█████████▌| 48/50 [00:45<00:01,  1.48it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  49\n"]},{"name":"stderr","output_type":"stream","text":["\r 98%|█████████▊| 49/50 [00:46<00:00,  1.35it/s]"]},{"name":"stdout","output_type":"stream","text":["Collecting trajectory  50\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 50/50 [00:47<00:00,  1.06it/s]\n"]}],"source":["### Collecting trajectories (i.e. demonstrations) using the expert policy\n","NUM_TRAJS = 50\n","observations, actions = [], []\n","reseed(1)\n","for traj_num in tqdm(range(NUM_TRAJS)):\n","    print(\"Collecting trajectory \", traj_num+1)\n","    done = False\n","    obs = env.reset(seed = 1)\n","    while not done:\n","        with torch.no_grad():\n","            action = expert.get_expert_action(obs)\n","            observations.append(obs)\n","            actions.append(action)\n","            obs, reward, done, info = env.step(action)\n","        if done:\n","            break"]},{"cell_type":"markdown","metadata":{"id":"xeEgG8SROlnV"},"source":["# **Q1: Behavior Cloning (BC) with Shaky Hands**\n","\n","To begin, fill in the implementation for the training loop function in **bc.py** found in **A2_FILES**. We provide the loss function and optimizer already, just iterate through your dataloader and return the updated policy!\n","\n","Once you finish the training loop implementation, it is now time to build up your agents! **Behavior cloning (BC)** is the simplest imitation learning algorithm, where we perform supervised learning on the given (offline) expert dataset. We either do this via log-likelihood maximization (cross-entropy minimization) in the discrete action case, or mean-squared error minimization (can also do MLE) in the continuous control setting.\n","\n","If implemented correctly, training your BC model should take roughly 15 minutes."]},{"cell_type":"markdown","metadata":{"id":"QwiX5_1FOlnV"},"source":["### Train Behavior Cloning (BC) Model"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1709318475925,"user":{"displayName":"Jonathan Hu","userId":"02477033161893350208"},"user_tz":300},"id":"_sss1NFtuSJq"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o2Zc6z3wOlnV","outputId":"1ad3cee5-b0eb-47e7-8c88-8fea52363a9e"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training:   9%|▊         | 129/1500 [01:07<11:00,  2.08it/s]"]}],"source":["import bc\n","\n","bc_learner = Learner(env)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","bc_learner.to(device)\n","checkpoint_path = get_checkpoint_path(\"bc\")\n","reseed(2)\n","res = bc.train(bc_learner, observations, actions, checkpoint_path, num_epochs = 1500)"]},{"cell_type":"markdown","metadata":{"id":"A5M_zcKdOlnV"},"source":["### Visualize the learner policy and compare rewards with expert policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uk8_YxMROlnV"},"outputs":[],"source":["done = False\n","visualize = False # set to false in order to disable rendering code\n","reseed(2)\n","obs = env.reset(seed = 2)\n","total_learner_reward = 0\n","i= 0\n","while not done:\n","    if i%20==0 and visualize:\n","        ipythondisplay.clear_output(wait=True)\n","        screen = env.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","    with torch.no_grad():\n","        action = bc_learner.get_action(obs)\n","    obs, reward, done, info = env.step(action)\n","    total_learner_reward += reward\n","    if done:\n","        break\n","print(f\"Total Reward using Expert Policy = {total_expert_reward}\\nTotal Reward using Learned Policy = {total_learner_reward}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"0ML4AMGcOlnV"},"source":["**Approximate expected reward for total reward using learned policy: 1000**"]},{"cell_type":"markdown","metadata":{"id":"BIoYlPSZOlnW"},"source":["Most likely, the performance of your BC agent will be very close to the expert.  However, what happens if your learner has SHAKY HANDS, i.e it executes random actions every few timesteps?\n","\n","Concretely, set the probability of a random action by the learner to be just 5% (code already provided). You will probably see that the performance of the learner tanks!"]},{"cell_type":"markdown","metadata":{"id":"HK8Jvb5aOlnW"},"source":["### Add 1% random actions to learner and check rewards"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LlUaa3FxOlnW"},"outputs":[],"source":["bc_learner.random_prob = 0.05\n","checkpoint_path = \"random_acts\"\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","bc_learner.to(device)\n","reseed(2)\n","bc.train(bc_learner, observations, actions, checkpoint_path, num_epochs = 1500)"]},{"cell_type":"markdown","metadata":{"id":"IB5GEzbsOlnW"},"source":["### Visualize learner policy with random actions and compare rewards with expert policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xYk6OWsCOlnW"},"outputs":[],"source":["done = False\n","visualize = False # set to false in order to disable rendering code\n","reseed(2)\n","obs = env.reset(seed=2)\n","total_learner_reward = 0\n","i= 0\n","while not done:\n","    if i%5==0 and visualize:\n","        screen = env.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","        ipythondisplay.clear_output(wait=True)\n","    with torch.no_grad():\n","        action = bc_learner.get_action(obs)\n","    obs, reward, done, info = env.step(action)\n","    total_learner_reward += reward\n","    if done:\n","        break\n","print(f\"Total Reward using Expert Policy = {total_expert_reward}\\nTotal Reward using Learned Policy (Random Actions)= {total_learner_reward}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"IVjj9yJQOlnW"},"source":["**Approximate expected reward for total reward using learned policy with 1% random actions: 111**"]},{"cell_type":"markdown","metadata":{"id":"wSAXJA7nOlnX"},"source":["# **Q2: DAgger**\n","\n","**Dataset aggregation (DAgger)** is a fundamentally interactive algorithm, where we can query the expert any time we want to get information about how to proceed. This allows for significantly more freedom for the learner, as it can ask the expert anywhere and not be limited by the dataset that it is given to learn from.\n","\n","**Can we overcome shaky hands with DAgger?** Fundamentally, this algorithm allows the learner to recover from bad states and should lead to much better performance than simply behavior cloning a fixed set of expert demonstrations. For this portion of the assignment, you will interact with the environment using the learner policy with random actions. You will do so in **dagger.py** found in **A2_FILES**.\n","\n","Remember to initialize the DAgger policy with the already learned BC policy and your dataset with the already collected expert demonstrations for BC.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AiFfUOghOlnX"},"source":["### Initialize DAgger with BC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJWksP47Olnf"},"outputs":[],"source":["dagger_learner = Learner(env)\n","dagger_learner.random_prob = 0.05\n","dagger_learner.load_state_dict(torch.load(get_checkpoint_path(\"bc\")))\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","dagger_learner.to(device)"]},{"cell_type":"markdown","metadata":{"id":"Mdpt6yXAOlnf"},"source":["### Interact with the environment using the learner policy with random actions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N65shAw7Olnf"},"outputs":[],"source":["import dagger\n","\n","observations, actions = [], [] # this has already been initialized earlier, why set to empty\n","checkpoint_path = get_checkpoint_path(\"dagger\")\n","seed = 2\n","reseed(seed)\n","dagger.interact(env, dagger_learner, expert, observations, actions, checkpoint_path, seed, num_epochs = 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nimGEXQYCSAw"},"outputs":[],"source":["dagger_learner.load_state_dict(torch.load(get_checkpoint_path(\"dagger\")))\n","dagger_learner_last_interaction = Learner(env)\n","dagger_learner_last_interaction.load_state_dict(torch.load(\"dagger_last_epoch\"))\n","done = False\n","visualize = False # set to false in order to disable rendering code\n","reseed(2)\n","obs = env.reset(seed=2)\n","total_best_learner_reward = 0\n","total_last_interaction_reward = 0\n","i= 0\n","while not done:\n","    if i%5==0 and visualize:\n","        screen = env.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","        ipythondisplay.clear_output(wait=True)\n","    with torch.no_grad():\n","        action = dagger_learner.get_action(obs)\n","    obs, reward, done, info = env.step(action)\n","    total_best_learner_reward += reward\n","    if done:\n","        break\n","while not done:\n","    if i%5==0 and visualize:\n","        screen = env.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","        ipythondisplay.clear_output(wait=True)\n","    with torch.no_grad():\n","        action = dagger_learner_last_interaction.get_action(obs)\n","    obs, reward, done, info = env.step(action)\n","    total_last_interaction_reward += reward\n","    if done:\n","        break\n","print(f\"Total Reward using Expert Policy = {total_expert_reward}\\nTotal Reward using Learned Best Policy (Dagger)= {total_learner_reward}\\nTotal Reward using Learned Last Policy (Dagger)= {total_last_interaction_reward}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"khXN_z9gRDaU"},"outputs":[],"source":["done = False\n","visualize = False # set to false in order to disable rendering code\n","reseed(3)\n","obs = env.reset(seed=3)\n","total_best_learner_reward = 0\n","total_last_interaction_reward = 0\n","i= 0\n","while not done:\n","    if i%5==0 and visualize:\n","        screen = env.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","        ipythondisplay.clear_output(wait=True)\n","    with torch.no_grad():\n","        action = dagger_learner.get_action(obs)\n","    obs, reward, done, info = env.step(action)\n","    total_best_learner_reward += reward\n","    if done:\n","        break\n","while not done:\n","    if i%5==0 and visualize:\n","        screen = env.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","        ipythondisplay.clear_output(wait=True)\n","    with torch.no_grad():\n","        action = dagger_learner_last_interaction.get_action(obs)\n","    obs, reward, done, info = env.step(action)\n","    total_last_interaction_reward += reward\n","    if done:\n","        break\n","print(f\"Total Reward using Expert Policy = {total_expert_reward}\\nTotal Reward using Learned Best Policy (Dagger)= {total_learner_reward}\\nTotal Reward using Learned Last Policy (Dagger)= {total_last_interaction_reward}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6_3jHS5RcOe"},"outputs":[],"source":["done = False\n","visualize = False # set to false in order to disable rendering code\n","reseed(4)\n","obs = env.reset(seed=4)\n","total_best_learner_reward = 0\n","total_last_interaction_reward = 0\n","i= 0\n","while not done:\n","    if i%5==0 and visualize:\n","        screen = env.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","        ipythondisplay.clear_output(wait=True)\n","    with torch.no_grad():\n","        action = dagger_learner.get_action(obs)\n","    obs, reward, done, info = env.step(action)\n","    total_best_learner_reward += reward\n","    if done:\n","        break\n","while not done:\n","    if i%5==0 and visualize:\n","        screen = env.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","        ipythondisplay.clear_output(wait=True)\n","    with torch.no_grad():\n","        action = dagger_learner_last_interaction.get_action(obs)\n","    obs, reward, done, info = env.step(action)\n","    total_last_interaction_reward += reward\n","    if done:\n","        break\n","print(f\"Total Reward using Expert Policy = {total_expert_reward}\\nTotal Reward using Learned Best Policy (Dagger)= {total_learner_reward}\\nTotal Reward using Learned Last Policy (Dagger)= {total_last_interaction_reward}\")"]},{"cell_type":"markdown","metadata":{"id":"UMPHa-uUOlnf"},"source":[":**Approximate expected reward for 50 interaction with the environment: 590**"]},{"cell_type":"markdown","metadata":{"id":"7CRaWar6Olng"},"source":["# **Extra Credit: Causal Confounds**\n","\n","Congratulations, you made it! You have implemented your first few (“deep” :') ) imitation learning algorithms in PyTorch.\n","\n","With that in mind, let’s dig a little deeper. A common problem in the real world is hidden information. What if parts of the robot's state are hidden from the learner? How well does imitation learning do when the expert has full state knowledge, but the learner does not?\n","\n","You will need to:\n","* Create a “partially observable” Hopper environment where the last observation index (refer to Gym documentation) is hidden from the learner (note that it’s still available to the expert!)\n","* Obtain rewards for both BC and DAgger. How well do BC and DAgger work for the partially observable Hopper environment? Explain the performance of each.\n","\n","**Note:** For this part, BC and DAgger should just work if you did things right.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tG4VEf0SMkH8"},"outputs":[],"source":["class Learner_hidden(nn.Module):\n","    def __init__(self, env, hidden_dim = 256, random_prob=0.0):\n","        super().__init__()\n","        self.fc1 = nn.Linear(10, hidden_dim) # 10 because one index from observation is hidden\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc_out = nn.Linear(hidden_dim, np.prod(env.action_space.shape))\n","\n","        self.env = env\n","        self.random_prob = random_prob\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        out = F.tanh(self.fc_out(x))\n","        return out\n","\n","    def get_action(self, obs):\n","        if np.random.random() < self.random_prob:\n","            return self.env.action_space.sample()\n","        # print(torch.tensor([obs]).device())\n","        action = self.forward(torch.tensor([obs]).float().to(device))\n","        return np.array(action[0].detach().cpu())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grghie29LJCT"},"outputs":[],"source":["### Collecting trajectories (i.e. demonstrations) using the expert policy\n","NUM_TRAJS = 50\n","observations, actions = [], []\n","reseed(1)\n","for traj_num in tqdm(range(NUM_TRAJS)):\n","    print(\"Collecting trajectory \", traj_num+1)\n","    done = False\n","    obs = env.reset(seed = 1)\n","    while not done:\n","        with torch.no_grad():\n","            action = expert.get_expert_action(obs)\n","            observations.append(obs[:-1])\n","            actions.append(action)\n","            obs, reward, done, info = env.step(action)\n","        if done:\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yu8T5MdELUE8"},"outputs":[],"source":["print(len(observations[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXik4SkqJe4G"},"outputs":[],"source":["bc_learner_hidden = Learner_hidden(env)\n","bc_learner_hidden.random_prob = 0.05\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","bc_learner_hidden.to(device)\n","\n","reseed(2)\n","bc.train(bc_learner_hidden, observations, actions, \"bc_hidden\", num_epochs = 1500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FHySp-rRm_d"},"outputs":[],"source":["done = False\n","visualize = False # set to false in order to disable rendering code\n","reseed(2)\n","obs = env.reset(seed = 2)\n","total_learner_reward = 0\n","i= 0\n","while not done:\n","    if i%20==0 and visualize:\n","        ipythondisplay.clear_output(wait=True)\n","        screen = env.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","    with torch.no_grad():\n","        action = bc_learner_hidden.get_action(obs[:-1])\n","    obs, reward, done, info = env.step(action)\n","    total_learner_reward += reward\n","    if done:\n","        break\n","print(f\"Total Reward using Expert Policy = {total_expert_reward}\\nTotal Reward using Learned Policy = {total_learner_reward}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxHZrertLTAu"},"outputs":[],"source":["import bc\n","import torch\n","\n","def interact_hidden(env, learner, expert, observations, actions, checkpoint_path, seed, num_epochs=100):\n","  \"\"\"Interact with the environment and update the learner policy using DAgger.\n","\n","    This function interacts with the given Gym environment and aggregates to\n","    the BC dataset by querying the expert.\n","\n","    Parameters:\n","        env (Env)\n","            The gym environment (in this case, the Hopper gym environment)\n","        learner (Learner)\n","            A Learner object (policy)\n","        expert (ExpertActor)\n","            An ExpertActor object (expert policy)\n","        observations (list of numpy.ndarray)\n","            An initially empty list of numpy arrays\n","        actions (list of numpy.ndarray)\n","            An initially empty list of numpy arrays\n","        checkpoint_path (str)\n","            The path to save the best performing model checkpoint\n","        seed (int)\n","            The seed to use for the environment\n","        num_epochs (int)\n","            Number of epochs to run the train function for\n","    \"\"\"\n","  # Interact with the environment and aggregate your BC Dataset by querying the expert\n","  NUM_INTERACTIONS = 50\n","  best_reward = float(\"-inf\")\n","  best_state_dict = None\n","  for episode in range(NUM_INTERACTIONS):\n","      total_learner_reward = 0\n","      done = False\n","      obs = env.reset(seed=seed)\n","      while not done:\n","        # TODO: Implement Hopper environment interaction and dataset aggregation here\n","        with torch.no_grad():\n","            learner_action = learner.get_action(obs[:-1])\n","            expert_action = expert.get_expert_action(obs)\n","            observations.append(obs[:-1])\n","\n","            # aggregate new expert action\n","            actions.append(expert_action)\n","            obs, reward, done, info = env.step(learner_action)\n","            total_learner_reward += reward\n","        if done:\n","          break\n","      print(f\"After interaction {episode}, reward = {total_learner_reward}\")\n","      if total_learner_reward > best_reward:\n","        best_reward = total_learner_reward\n","        best_state_dict = learner.state_dict()\n","      bc.train(learner, observations, actions, \"dagger_hidden_last_epoch\", num_epochs)\n","\n","  torch.save(best_state_dict, checkpoint_path)\n","  learner.load_state_dict(best_state_dict)\n","\n","  return learner"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2d0fDRLVPGtt"},"outputs":[],"source":["dagger_learner_hidden = Learner_hidden(env)\n","dagger_learner_hidden.random_prob = 0.05\n","dagger_learner_hidden.load_state_dict(torch.load(\"bc_hidden\"))\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","dagger_learner_hidden.to(device)\n","\n","observations, actions = [], []\n","seed = 2\n","reseed(seed)\n","interact_hidden(env, dagger_learner_hidden, expert, observations, actions, \"dagger_hidden\", seed, num_epochs = 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YRbvQ3uARxVx"},"outputs":[],"source":["done = False\n","visualize = False # set to false in order to disable rendering code\n","reseed(2)\n","obs = env.reset(seed = 2)\n","total_learner_reward = 0\n","i= 0\n","while not done:\n","    if i%20==0 and visualize:\n","        ipythondisplay.clear_output(wait=True)\n","        screen = env.render()\n","        plt.imshow(screen[0])\n","        ipythondisplay.display(plt.gcf())\n","    with torch.no_grad():\n","        action = dagger_learner_hidden.get_action(obs)\n","    obs, reward, done, info = env.step(action)\n","    total_learner_reward += reward\n","    if done:\n","        break\n","print(f\"Total Reward using Expert Policy = {total_expert_reward}\\nTotal Reward using Learned Policy = {total_learner_reward}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"cVd8NOHoRftu"},"source":["\n","1.   How do they do in comparison to the fully observable environments? Why is this the case?\n","2.   How do they do in comparison to one another? Why is this the case?\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8riK4BOR0sW"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
