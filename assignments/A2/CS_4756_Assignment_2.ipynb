{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caVTy-IeOlnM"
      },
      "source": [
        "### **Due Date**\n",
        "2/29/2024 at 11:59PM EST\n",
        "\n",
        "# **Introduction**\n",
        "\n",
        "Welcome to Assignment 2 of 4756. In this assignment, you will train an agent using demonstrations from an expert. Concretely, you will:\n",
        "* Implement behavior cloning (BC) and dataset aggregation (DAgger) methods\n",
        "* **Extra Credit:** Get imitation learning working under causal confounds\n",
        "\n",
        "You will use the Hopper agent for this assignment, which is part of Gym’s Mujoco Environments. Refer to the Gym website for more details about the [Hopper environment](https://gymnasium.farama.org/environments/mujoco/hopper/).\n",
        "\n",
        "\n",
        "Please read through the following paragraphs carefully, as they will apply to this and all future assignments.\n",
        "\n",
        "**Getting Started:** This assignment should be completed in [Google Colab](https://colab.research.google.com/). In order to access the python files bc.py and dagger.py which you will be editing, it is necessary to first upload the folder A2_FILES to your google drive and then mount your Google Drive in Colab. To do so, carefully follow the directions below in the section **Mounting Google Drive to Colab**, or reference the instructions [here](https://saturncloud.io/blog/how-to-import-python-files-in-google-colaboratory/). Additionally, make sure to switch your runtime type to GPU; this will help speed up the training process.\n",
        "\n",
        "**Evaluation:**\n",
        "Your code will be tested for correctness and, for certain assignments, speed. For this particular assignment, performance results will not be harshly graded (although we provide approximate expected reward numbers as lower bounds, you are not expected to replicate them exactly); however, it will be important to make an effort to justify your approach which led to the obtained results. Please remember that all assignments should be completed individually.\n",
        "\n",
        "**Academic Integrity:** We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else’s code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don’t try. We trust you all to submit your own work only; please don’t let us down. If you do, we will pursue the strongest consequences available to us.\n",
        "\n",
        "**Getting Help:** The [Resources](https://www.cs.cornell.edu/courses/cs4756/2024sp/#resources) section on the course website is your friend! If you ever feel stuck in these projects, please feel free to avail yourself to office hours and Edstem! If you are unable to make any of the office hours listed, please let TAs know and we will be happy to assist. If you need a refresher for PyTorch, please see this [60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)! For Numpy, please see the quickstart [here](https://numpy.org/doc/stable/user/quickstart.html) and full API [here](https://numpy.org/doc/stable/reference/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix2ll62jOlnP"
      },
      "source": [
        "### **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr_wTb2DOlnQ"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf\n",
        "!pip install gym\n",
        "\n",
        "!pip install free-mujoco-py\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install imageio==2.4.1\n",
        "!pip install -U colabgymrender\n",
        "!pip install mujoco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDT0P3a5OlnQ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import optimizer\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8ZJJP4ivZl_"
      },
      "outputs": [],
      "source": [
        "# Setting the seed to ensure reproducability\n",
        "def reseed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "reseed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdqs6TQEOlnR"
      },
      "source": [
        "### **Mounting Google Drive in Colab**\n",
        "\n",
        "Before you complete this step, make sure that you have uploaded the folder A2_FILES to your Google Drive. Once you have done that, you need to mount your Google Drive in Colab. In order to do so, run the cell below. Running this cell will prompt you to authorize Colab to access your drive. Follow the instructions to complete the authorization process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB8NMNhXOlnR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2a5e9TgOlnR"
      },
      "source": [
        "Next, locate A2_FILES on the left panel in Colab. To do so, navigate to Files/drive/MyDrive. At this point, you should see the contents of your Google Drive. Locate A2_FILES in your drive, and if necessary, modify the cell below such that you are correctly indicating the file path to A2_FILES. You will append the path to A2_FILES to the system path. If you have completed this step correctly, you should be able to successfully import the BC and DAgger modules into this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knBKVIwKOlnR"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/A2_FILES')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srFKEtW3OlnS"
      },
      "source": [
        "### **Setting Up the Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1HBxcIKOlnS"
      },
      "outputs": [],
      "source": [
        "def make_env(env_id, seed=42, p_tremble=0.0):\n",
        "    env = gym.make(env_id, render_mode=None) # Change render_mode = rbg_array to render\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    return env\n",
        "env = make_env('Hopper-v3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxEY3uf3OlnS"
      },
      "source": [
        "### **Visualizing the Hopper environment with random actions**\n",
        "\n",
        "We have provided functions to visualize the environment and compute rewards on the Hopper environment with random actions. Looking through this code will help you get familiarized with the environment, and set you up for the next parts in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acXcvL5fOlnS"
      },
      "outputs": [],
      "source": [
        "plt.axis('off')\n",
        "done = False\n",
        "visualize = False # set to false in order to disable rendering code\n",
        "obs = env.reset()\n",
        "total_random_reward = 0\n",
        "i = 0\n",
        "while not done:\n",
        "    i += 1\n",
        "    if i%5==0 and visualize:\n",
        "        ipythondisplay.clear_output(wait=True)\n",
        "        screen = env.render()\n",
        "        plt.imshow(screen[0])\n",
        "        ipythondisplay.display(plt.gcf())\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    total_random_reward += reward\n",
        "    if done:\n",
        "        break\n",
        "print(\"Total Reward using Random Actions = \", total_random_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FihI62JwOlnS"
      },
      "source": [
        "**Approximate expected reward for total reward using random actions: 27**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoHvUiacOlnT"
      },
      "outputs": [],
      "source": [
        "# Download Hopper expert policy\n",
        "!wget https://github.com/portal-cornell/cs4756-robot-learning-sp24/raw/main/assignments/A2/experts/hopper.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbtKTFMbOlnT"
      },
      "source": [
        "### **Neural Networks in PyTorch**\n",
        "\n",
        "We have provided some code for implementing simple neural networks (fully connected, multilayer perceptrons) in PyTorch, including the ExpertActor and Learner classes. We have also provided code for checkpointing for saving your best performing model. If you wish to learn more about how to construct and train neural networks in PyTorch, check out the tutorials on [pytorch.org](https://pytorch.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw_w17lhOlnT"
      },
      "source": [
        "### ExpertActor Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khjGbIfJOlnT"
      },
      "outputs": [],
      "source": [
        "LOG_STD_MAX = 2\n",
        "LOG_STD_MIN = -5\n",
        "\n",
        "class ExpertActor(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod(), 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc_mean = nn.Linear(256, np.prod(env.action_space.shape))\n",
        "        self.fc_logstd = nn.Linear(256, np.prod(env.action_space.shape))\n",
        "        # action rescaling\n",
        "        self.register_buffer(\n",
        "            \"action_scale\",\n",
        "            torch.tensor(\n",
        "                (env.action_space.high - env.action_space.low) / 2.0,\n",
        "                dtype=torch.float32,\n",
        "            ).reshape(1, -1),\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"action_bias\",\n",
        "            torch.tensor(\n",
        "                (env.action_space.high + env.action_space.low) / 2.0,\n",
        "                dtype=torch.float32,\n",
        "            ).reshape(1, -1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        mean = self.fc_mean(x)\n",
        "        log_std = self.fc_logstd(x)\n",
        "        log_std = torch.tanh(log_std)\n",
        "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (\n",
        "            log_std + 1\n",
        "        )\n",
        "\n",
        "        return mean, log_std\n",
        "\n",
        "    def get_action(self, x):\n",
        "        mean, log_std = self(x)\n",
        "        std = log_std.exp()\n",
        "        normal = torch.distributions.Normal(mean, std)\n",
        "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
        "        y_t = torch.tanh(x_t)\n",
        "        action = y_t * self.action_scale + self.action_bias\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "        # Enforcing Action Bound\n",
        "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\n",
        "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
        "        return action, log_prob, mean\n",
        "\n",
        "    def get_expert_action(self, obs, random_prob=0.0):\n",
        "        if np.random.random() < random_prob:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            action = self.get_action(torch.tensor([obs]).float())\n",
        "            return np.array(action[0][0].detach().cpu())\n",
        "\n",
        "ckpt_path = \"hopper.pt\"\n",
        "expert = ExpertActor(env).to('cpu')\n",
        "expert.load_state_dict(torch.load(str(ckpt_path), map_location='cpu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR2vvUHrOlnT"
      },
      "source": [
        "### Learner Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiZA33etOlnT"
      },
      "outputs": [],
      "source": [
        "class Learner(nn.Module):\n",
        "    def __init__(self, env, hidden_dim = 256, random_prob=0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod(), hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_out = nn.Linear(hidden_dim, np.prod(env.action_space.shape))\n",
        "\n",
        "        self.env = env\n",
        "        self.random_prob = random_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        out = F.tanh(self.fc_out(x))\n",
        "        return out\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        if np.random.random() < self.random_prob:\n",
        "            return self.env.action_space.sample()\n",
        "        action = self.forward(torch.tensor([obs]).float())\n",
        "        return np.array(action[0].detach().cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1t7PJjCUlr7"
      },
      "source": [
        "### Checkpointing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJe4atJ1UkFU"
      },
      "outputs": [],
      "source": [
        "def get_checkpoint_path(algo):\n",
        "    \"\"\"Return the path to save the best performing model checkpoint.\n",
        "\n",
        "    Parameters:\n",
        "        algo (str)\n",
        "          Indicates which algorithm will be used to train the model\n",
        "\n",
        "    Returns:\n",
        "        checkpoint_path (str)\n",
        "            The path to save the best performing model checkpoint\n",
        "    \"\"\"\n",
        "    if algo == \"bc\":\n",
        "      return 'best_bc_checkpoint.pth'\n",
        "    elif algo == \"dagger\":\n",
        "      return 'best_dagger_checkpoint.pth'\n",
        "    return 'best_model_checkpoint.pth'\n",
        "\n",
        "def load_model_checkpoint(checkpoint_path):\n",
        "    \"\"\"Load a model checkpoint from disk.\n",
        "\n",
        "    Parameters:\n",
        "        checkpoint_path (str)\n",
        "            The path to load the checkpoint from\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module)\n",
        "            The model loaded from the checkpoint\n",
        "    \"\"\"\n",
        "    model = Learner(env)\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yru8Y-CFOlnU"
      },
      "source": [
        "### **Visualizing the Hopper environment with the expert policy**\n",
        "\n",
        "We have provided a visualization for computing rewards using the expert policy on the Hopper environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpATVTRdOlnU"
      },
      "outputs": [],
      "source": [
        "plt.axis('off')\n",
        "done = False\n",
        "visualize = False # set to false in order to disable rendering code\n",
        "reseed(1)\n",
        "obs = env.reset(seed=1)\n",
        "total_expert_reward = 0\n",
        "i = 0\n",
        "while not done:\n",
        "    i += 1\n",
        "    if i%20==0 and visualize:\n",
        "        ipythondisplay.clear_output(wait=True)\n",
        "        screen = env.render()\n",
        "        plt.imshow(screen[0])\n",
        "        ipythondisplay.display(plt.gcf())\n",
        "    with torch.no_grad():\n",
        "        action = expert.get_expert_action(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    total_expert_reward += reward\n",
        "    if done:\n",
        "        break\n",
        "print(f\"Total Reward using Expert Policy = {total_expert_reward}\\nTotal Reward using Random Actions = {total_random_reward}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDhO0BKCOlnU"
      },
      "source": [
        "**Approximate expected reward for total reward using expert policy: 2238**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-4h88CAOlnU"
      },
      "source": [
        "### **Data collection**\n",
        "\n",
        "We have provided some code to collect 50 demonstrations using the expert policy. To collect a different number of  trajectories, change the value of the NUM_TRAJS variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvy7CL5kOlnU"
      },
      "source": [
        "### Collecting and processing offline data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsfopEx_OlnV"
      },
      "outputs": [],
      "source": [
        "### Collecting trajectories (i.e. demonstrations) using the expert policy\n",
        "NUM_TRAJS = 50\n",
        "observations, actions = [], []\n",
        "reseed(1)\n",
        "for traj_num in tqdm(range(NUM_TRAJS)):\n",
        "    print(\"Collecting trajectory \", traj_num+1)\n",
        "    done = False\n",
        "    obs = env.reset(seed = 1)\n",
        "    while not done:\n",
        "        with torch.no_grad():\n",
        "            action = expert.get_expert_action(obs)\n",
        "            observations.append(obs)\n",
        "            actions.append(action)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeEgG8SROlnV"
      },
      "source": [
        "# **Q1: Behavior Cloning (BC) with Shaky Hands**\n",
        "\n",
        "To begin, fill in the implementation for the training loop function in **bc.py** found in **A2_FILES**. We provide the loss function and optimizer already, just iterate through your dataloader and return the updated policy!\n",
        "\n",
        "Once you finish the training loop implementation, it is now time to build up your agents! **Behavior cloning (BC)** is the simplest imitation learning algorithm, where we perform supervised learning on the given (offline) expert dataset. We either do this via log-likelihood maximization (cross-entropy minimization) in the discrete action case, or mean-squared error minimization (can also do MLE) in the continuous control setting.\n",
        "\n",
        "If implemented correctly, training your BC model should take roughly 15 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwiX5_1FOlnV"
      },
      "source": [
        "### Train Behavior Cloning (BC) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2Zc6z3wOlnV"
      },
      "outputs": [],
      "source": [
        "import bc\n",
        "\n",
        "bc_learner = Learner(env)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "bc_learner.to(device)\n",
        "checkpoint_path = get_checkpoint_path(\"bc\")\n",
        "reseed(2)\n",
        "bc.train(bc_learner, observations, actions, checkpoint_path, num_epochs = 1500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5M_zcKdOlnV"
      },
      "source": [
        "### Visualize the learner policy and compare rewards with expert policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk8_YxMROlnV"
      },
      "outputs": [],
      "source": [
        "done = False\n",
        "visualize = False # set to false in order to disable rendering code\n",
        "reseed(2)\n",
        "obs = env.reset(seed = 2)\n",
        "total_learner_reward = 0\n",
        "i= 0\n",
        "while not done:\n",
        "    if i%20==0 and visualize:\n",
        "        ipythondisplay.clear_output(wait=True)\n",
        "        screen = env.render()\n",
        "        plt.imshow(screen[0])\n",
        "        ipythondisplay.display(plt.gcf())\n",
        "    with torch.no_grad():\n",
        "        action = bc_learner.get_action(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    total_learner_reward += reward\n",
        "    if done:\n",
        "        break\n",
        "print(f\"Total Reward using Expert Policy = {total_expert_reward}\\nTotal Reward using Learned Policy = {total_learner_reward}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ML4AMGcOlnV"
      },
      "source": [
        "**Approximate expected reward for total reward using learned policy: 1000**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIoYlPSZOlnW"
      },
      "source": [
        "Most likely, the performance of your BC agent will be very close to the expert.  However, what happens if your learner has SHAKY HANDS, i.e it executes random actions every few timesteps?\n",
        "\n",
        "Concretely, set the probability of a random action by the learner to be just 5% (code already provided). You will probably see that the performance of the learner tanks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK8Jvb5aOlnW"
      },
      "source": [
        "### Add 1% random actions to learner and check rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlUaa3FxOlnW"
      },
      "outputs": [],
      "source": [
        "bc_learner.random_prob = 0.05\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "bc_learner.to(device)\n",
        "reseed(2)\n",
        "bc.train(bc_learner, observations, actions, checkpoint_path, num_epochs = 1500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB5GEzbsOlnW"
      },
      "source": [
        "### Visualize learner policy with random actions and compare rewards with expert policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYk6OWsCOlnW"
      },
      "outputs": [],
      "source": [
        "done = False\n",
        "visualize = False # set to false in order to disable rendering code\n",
        "reseed(2)\n",
        "obs = env.reset(seed=2)\n",
        "total_learner_reward = 0\n",
        "i= 0\n",
        "while not done:\n",
        "    if i%5==0 and visualize:\n",
        "        screen = env.render()\n",
        "        plt.imshow(screen[0])\n",
        "        ipythondisplay.display(plt.gcf())\n",
        "        ipythondisplay.clear_output(wait=True)\n",
        "    with torch.no_grad():\n",
        "        action = bc_learner.get_action(obs)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    total_learner_reward += reward\n",
        "    if done:\n",
        "        break\n",
        "print(f\"Total Reward using Expert Policy = {total_expert_reward}\\nTotal Reward using Learned Policy (Random Actions)= {total_learner_reward}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVjj9yJQOlnW"
      },
      "source": [
        "**Approximate expected reward for total reward using learned policy with 1% random actions: 111**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSAXJA7nOlnX"
      },
      "source": [
        "# **Q2: DAgger**\n",
        "\n",
        "**Dataset aggregation (DAgger)** is a fundamentally interactive algorithm, where we can query the expert any time we want to get information about how to proceed. This allows for significantly more freedom for the learner, as it can ask the expert anywhere and not be limited by the dataset that it is given to learn from.\n",
        "\n",
        "**Can we overcome shaky hands with DAgger?** Fundamentally, this algorithm allows the learner to recover from bad states and should lead to much better performance than simply behavior cloning a fixed set of expert demonstrations. For this portion of the assignment, you will interact with the environment using the learner policy with random actions. You will do so in **dagger.py** found in **A2_FILES**.\n",
        "\n",
        "Remember to initialize the DAgger policy with the already learned BC policy and your dataset with the already collected expert demonstrations for BC.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiFfUOghOlnX"
      },
      "source": [
        "### Initialize DAgger with BC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJWksP47Olnf"
      },
      "outputs": [],
      "source": [
        "dagger_learner = bc_learner\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "dagger_learner.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdpt6yXAOlnf"
      },
      "source": [
        "### Interact with the environment using the learner policy with random actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N65shAw7Olnf"
      },
      "outputs": [],
      "source": [
        "import dagger\n",
        "\n",
        "observations, actions = [], []\n",
        "checkpoint_path = get_checkpoint_path(\"dagger\")\n",
        "seed = 2\n",
        "reseed(seed)\n",
        "dagger.interact(env, dagger_learner, expert, observations, actions, checkpoint_path, seed, num_epochs = 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMPHa-uUOlnf"
      },
      "source": [
        "**Approximate expected reward for 100th interaction with the environment: 1328**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CRaWar6Olng"
      },
      "source": [
        "# **Extra Credit: Causal Confounds**\n",
        "\n",
        "Congratulations, you made it! You have implemented your first few (“deep” :') ) imitation learning algorithms in PyTorch.\n",
        "\n",
        "With that in mind, let’s dig a little deeper. A common problem in the real world is hidden information. What if parts of the robot's state are hidden from the learner? How well does imitation learning do when the expert has full state knowledge, but the learner does not?\n",
        "\n",
        "You will need to:\n",
        "* Create a “partially observable” Hopper environment where the last observation index (refer to Gym documentation) is hidden from the learner (note that it’s still available to the expert!)\n",
        "* Obtain rewards for both BC and DAgger. How well do BC and DAgger work for the partially observable Hopper environment? Explain the performance of each.\n",
        "\n",
        "**Note:** For this part, BC and DAgger should just work if you did things right.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
